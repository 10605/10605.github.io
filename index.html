<head>
    <meta charset="utf-8">
<title> CMU 10-405/10-605 </title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../docs-assets/ico/favicon.png">

    <!-- Bootstrap core CSS -->
    <link href="bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
</head>

<body>
    <div class="container">
	 <div class="panel-footer clearfix">
        <div class="pull-left">

            <h1>10-405/10-605: ML with Large Datasets, Spring 2021</h1>
            <p class="lead">Instructors:</p><p><a href=https://www.cs.cmu.edu/~smithv/ target="_blank">Prof. Virginia Smith</a><br /> <!--(OH: Wed 2pm-3pm, <a href="https://cmu.zoom.us/j/670566333">cmu.zoom.us/j/670566333)>-->
            <a href=https://www.cs.cmu.edu/~atalwalk/ target="_blank">Prof. Ameet Talwalkar</a></p>
            <p class="lead">Lecture:</p>
            <p>Mon/Wed 8:20am-9:40am
            </p>
            <p class="lead">Recitation:</p>
            <p>Fri 8:20am-9:40am</p>
            </div>
        </div>

        <div class="main">
        <br>
        <h3>Course Overview</h3>
        <p class="p1">Large datasets pose difficulties across the machine learning pipeline. They are difficult to visualize, and it can be hard to determine what sorts of errors and biases may be present in them. They are computationally expensive to process, and the cost of learning is often hard to predict---for instance, an algorithm that runs quickly on a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.<br /><br />This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the topics considered are: data cleaning, visualization, and pre-processing at scale; principles of parallel and distributed computing for machine learning; techniques for scalable deep learning; analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity; and methods for low-latency inference. Students will gain experience with common large-scale computing libraries and infrastructure, including Apache Spark and TensorFlow.
        </p>

        <h3>Prerequisites</h3>
        <p>Students are required to have taken a CMU introductory machine learning course (10-301, 10-315, 10-601, 10-701, or 10-715). A strong background in programming will also be necessary; suggested prerequisites include 15-210, 15-214, or equivalent. Students are expected to be familiar with Python or learn it during the course.</p>

        <h3>Textbooks</h3>
        There will be no required textbooks, though we may suggest additional reading in the schedule below.

        <h3>Piazza</h3>
        <p> We will use Piazza for class discussions. 
        Please go to <a href=https://piazza.com/cmu/fall2020/1060510805 target="_blank" >this Piazza website</a> to join the course forum (note: you must use a cmu.edu email account to join). 
        We strongly encourage students to post on this forum rather than emailing the course staff directly (this will be more efficient for both students and staff). Students should use Piazza to:
        <ul>
          <li>Ask clarifying questions about the course material.</li>
          <li>Share useful resources with classmates (so long as they do not
          contain homework solutions).</li>
          <li>Look for students to form study groups.</li>
          <li>Answer questions posted by other students to solidify your own
          understanding of the material.</li>
        </ul>
        The course Academic Integrity Policy must be followed on the message boards at all times. <b>Do not post or request homework solutions!</b> Also, please be polite.


        <h3>Acknowledgments</h3>
        <p> This course is based in part on material developed by <a href="https://heather.miller.am/" target="_blank">Heather Miller</a>, <a href="http://www.cs.cmu.edu/~wcohen/" target="_blank">William Cohen</a>, <a href="http://www.cs.berkeley.edu/~adj/" target="_blank">Anthony Joseph</a>, and <a href="http://www.cs.cmu.edu/~bapoczos/" target="_blank">Barnabas Poczos</a>.</p>
        <p>Previous courses: <a href="https://10605.github.io/fall2020/index.html" target="_blank">10-605/10-805, Fall 2020</a>; <a href="https://10605.github.io/spring2020/index.html" target="_blank">10-405/10-605, Spring 2020</a>.</p>
        <hr>


        <h3>Schedule (Subject to Change)</h3>
        <div class="table-responsive">
            <table class="table table-striped table-bordered table-hover">
                  <tbody>
                    <tr class="leading">
                        <th>Date</th>
                        <th>Lecture</th>
                        <th>Resources</th>
                        <th>Announcements</th>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Data Pre-Processing and Visualization, Distributed Computing</td>
                    </tr>
                    <tr>
                        <td>Feb 1</td>
                        <td>
                        Introduction
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 3</td>
                        <td>
                        Distributed Computing, Spark
                        </td>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 8</td>
                        <td>
                        Visualization, PCA
                        </td>
                        <td><a href="https://arxiv.org/pdf/1404.1100.pdf" target="_blank">Tutorial on PCA</a><br /><a href="https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf" target="_blank">JL Theorem</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 10</td>
                        <td>
                        Nonlinear Dimensionality Reduction
                        </td>
                        <td><a href="https://lvdmaaten.github.io/tsne/" target="_blank">t-SNE</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Basics of Large-Scale / Distributed Machine Learning</td>
                    </tr>
                    <tr>
                        <td>Feb 15</td>
                        <td>
                        Distributed Linear Regression, part I
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 17</td>
                        <td>
                        Distributed Linear Regression, part II
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 22</td>
                        <td>
                        Kernel Approximations
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 24</td>
                        <td>Guest Lecture
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 1</td>
                        <td>
                        Logistic Regression, Hashing
                        </td>
                        <td><a href="https://jmlr.csail.mit.edu/papers/volume10/shi09a/shi09a.pdf" target="_blank">Hash kernels, I</a><br /><a href="https://arxiv.org/pdf/0902.2206.pdf" target="_blank">Hash kernels, II</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 3</td>
                        <td>
                        Randomized Algorithms
                        </td>
                        <td><a href="http://dimacs.rutgers.edu/~graham/pubs/papers/cm-full.pdf" target="_blank">Count-min sketch</a><br /><a href="http://people.csail.mit.edu/indyk/p117-andoni.pdf" target="_blank">LSH</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 8</td>
                        <td>
                        Distributed Trees
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 10</td>
                        <td> TBA
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 15</td>
                        <td>
                        <b><i>Exam I</i></b>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 17</td>
                        <td>Cloud Computing &#38 Services
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Scalable Deep Learning: Training, Tuning, and Inference</td>
                    </tr>
                    <tr>
                        <td>Mar 22</td>
                        <td>Deep Learning, Autodiff
                        </td>
                        <td><a href="https://www.deeplearningbook.org/contents/mlp.html" target="_blank">Deep Learning, Ch. 6</a><br /><a href="https://www.tensorflow.org/tutorials/quickstart/beginner" target="_blank">TensorFlow Quickstart</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 25</td>
                        <td>
                        DL Frameworks, Hardware
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 29</td>
                        <td>
                        Large-Scale Optimization
                        </td>
                        <td><a href="https://arxiv.org/abs/1606.04838" target="_blank">Optimization for Large-Scale ML</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 31</td>
                        <td>
                        Optimization for DL
                        </td>
                        <td><a href="https://www.deeplearningbook.org/contents/optimization.html" target="_blank">Deep Learning, Ch. 8</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 5</td>
                        <td>
                        <b>No Class (CMU Break Day)</b>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 7</td>
                        <td>
                        Parallel/Distributed DL
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 12</td>
                        <td>
                        Hyperparameter Tuning
                        </td>
		                    <td><a href="https://homes.cs.washington.edu/~jamieson/hyperband.html" target="_blank">blog1</a>, <a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank">blog2</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 14</td>
                        <td>
                        Inference, Model Compression
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Advanced Topics</td>
                    </tr>
                    <tr>
                        <td>Apr 19</td>
                        <td>
                        Neural Architecture Search
                        </td>
                        <td><a href="https://blog.ml.cmu.edu/2020/07/17/in-defense-of-weight-sharing-for-nas/" target="_blank">blog</a>, <a href="https://arxiv.org/abs/1902.07638" target="_blank">RSWS</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 21</td>
                        <td> Guest Lecture
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 26</td>
                        <td>
                        Productionizing Large-Scale ML
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 28</td>
                        <td>
                        Federated Learning
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>May 3</td>
                        <td>
                        Course Summary
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>May 5</td>
                        <td>
                        <b><i>Exam II</i></b>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                </tbody>
            </table>
        </div>
        </div>

    </div>
    <!-- /container 


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>
    <script>
      anchors.options.visible = 'always';
      anchors.add('.anchored');
    </script>


</body>
