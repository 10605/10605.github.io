<head>
    <meta charset="utf-8">
<title> CMU 10-405/10-605 </title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../docs-assets/ico/favicon.png">

    <!-- Bootstrap core CSS -->
    <link href="bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
</head>

<body>
    <div class="container">
	 <div class="panel-footer clearfix">
        <div class="pull-left">

            <h1>10-405/10-605: ML with Large Datasets, Spring 2021</h1>
            <p class="lead">Instructors:</p><p><a href=https://www.cs.cmu.edu/~smithv/ target="_blank">Prof. Virginia Smith</a><br /> <!--(OH: Wed 2pm-3pm, <a href="https://cmu.zoom.us/j/670566333">cmu.zoom.us/j/670566333)>-->
            <a href=https://www.cs.cmu.edu/~atalwalk/ target="_blank">Prof. Ameet Talwalkar</a></p>
			<p class="lead">Education Associate:</p> <p>Daniel Bird (dpbird [at] Andrew [dot] cmu [dot] edu)</p>
            <p class="lead">Lecture:</p>
            <p>Mon/Wed 8:20am-9:40am
            </p>
            <p class="lead">Recitation:</p>
            <p>Fri 8:20am-9:40am</p>
            <p class="lead">Quick Links:</p>
            <p>[<a href="#overview">Overview</a>] [<a href="#components">Course Components</a>] [<a href="#technologies">Technologies</a>] [<a href="#schedule">Course Schedule</a>] [<a href="#policies">Policies</a>]</p>
            </div>
        </div>

        <div class="main">
        <br>
        <h3 id="overview">Course Overview</h3>
        <p class="p1">Large datasets pose difficulties across the machine learning pipeline. They are difficult to visualize and introduce computational, storage, and communication bottlenecks during data preprocessing and model training. Moreover, high capacity models often used in conjunction with large datasets introduce additional computational and storage hurdles during model training and inference. This course is intended to provide a student with the mathematical, algorithmic, and practical knowledge of issues involving learning with large datasets. Among the topics considered are: data cleaning, visualization, and pre-processing at scale; principles of parallel and distributed computing for machine learning; techniques for scalable deep learning; analysis of programs in terms of memory, computation, and (for parallel methods) communication complexity; and methods for low-latency inference.
        </p>

        <h4>Prerequisites</h4>
        <p>Students are required to have taken a CMU introductory machine learning course (10-301, 10-315, 10-601, 10-701, or 10-715). A strong background in programming will also be necessary; suggested prerequisites include 15-210, 15-214, or equivalent. Students are expected to be familiar with Python or learn it during the course.</p>

        <h4>Textbooks</h4>
        There will be no required textbooks, though we may suggest additional reading in the schedule below.
		
        <br /><br />
		<h3 id="components">Course Components</h3>
		The requirements of this course consist of participating in lectures, homework assignments, and two exams. The grading breakdown is the following:
		<ul> 
		<li>25% Exam 1 </li>
		<li>25% Exam 2 </li>
		<li>45% Homework (7 Assignments all weighted equally) </li>
		<li>5% Quizzes </li>
		</ul>
		
		<h4>Exams</h4>
		You are required to attend all exams. The exams will be given during class. Please plan your travel accordingly as we will not be able accommodate individual travel needs (e.g. by offering the exam early).<br /> <br />If you have an unavoidable conflict with an exam (e.g. an exam in another course), notify us by filling out the exam conflict form which will be released on Piazza a few weeks before the exam.
		
		<h4>Homework</h4>
		The homeworks will be divided into two components: programming and written. The programming assignments will ask you to implement ML algorithms from scratch; they emphasize understanding of real-world applications of ML, building end-to-end systems, and experimental design. The written assignments will focus on core concepts, “on-paper” implementations of classic learning algorithms, derivations, and understanding of theory.
        
		<br /><br />
        <h3 id="technologies">Technologies</h3>
		
		<h4>Piazza</h4>
        <p> We will use Piazza for class discussions. 
        Please go to <a href=https://piazza.com/cmu/spring2021/1040510605 target="_blank" >this Piazza website</a> to join the course forum (note: you must use a cmu.edu email account to join). 
        We strongly encourage students to post on this forum rather than emailing the course staff directly (this will be more efficient for both students and staff). Students should use Piazza to:
        <ul>
          <li>Ask clarifying questions about the course material.</li>
          <li>Share useful resources with classmates (so long as they do not
          contain homework solutions).</li>
          <li>Look for students to form study groups.</li>
          <li>Answer questions posted by other students to solidify your own
          understanding of the material.</li>
        </ul>
        The course Academic Integrity Policy must be followed on the message boards at all times. <b>Do not post or request homework solutions!</b> Also, please be polite.
		
		<h4>Gradescope</h4>
		We use Gradescope to collect PDF submissions of open-ended questions on the homework (e.g. mathematical derivations, plots, short answers). The course staff will manually grade your submission, and you’ll receive personalized feedback explaining your final marks.<br /> <br />You will also submit your code for programming questions on the homework to Gradescope. After uploading your code, our grading scripts will autograde your assignment by running your program on a VM. This provides you with immediate feedback on the performance of your submission.
		
		<h4>Regrade Requests</h4>
		If you believe an error was made during manual grading, you’ll be able to submit a regrade request on Gradescope. For each homework, regrade requests will be open for only 1 week after the grades have been published. This is to encourage you to check the feedback you’ve received early!
		
         <br /><br /><br />
        <h3 id="schedule">Schedule (Subject to Change)</h3>
        <div class="table-responsive">
            <table class="table table-striped table-bordered table-hover">
                  <tbody>
                    <tr class="leading">
                        <th>Date</th>
                        <th>Lecture</th>
                        <th>Resources</th>
                        <th>Announcements</th>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Data Pre-Processing and Visualization, Distributed Computing</td>
                    </tr>
                    <tr>
                        <td>Feb 1</td>
                        <td>
                        Introduction (<a href="https://drive.google.com/file/d/1oyKe9hPD0r9Yss9QKR0By6cJbksCo0gF/view?usp=sharing" target="_blank">slides</a>, <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=ef74249c-7d8d-4f6f-bc1c-acc200f8038f" target="_blank">video</a>)
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 3</td>
                        <td>
                        Distributed Computing, Spark (<a href="https://drive.google.com/file/d/1HOftjYTrmQHehucMW7lPmfCRHc3mhk0C/view?usp=sharing" target="_blank">slides</a>, <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1007b996-2d1f-465d-b153-acc401097ac4" target="_blank">video</a>)
                        </td>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
		    <tr>
                        <td><i>Feb 5</i></td>
                        <td>
                        <i> Recitation: Intro to Databricks, Spark (<a href="https://drive.google.com/file/d/1x_D2Um5pBaioY10v2zldy7EktCeP9mz5/view?usp=sharing" target="_blank">slides</a>, <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=88f75d1c-168e-4161-b852-acc6010e2b68" target="_blank">video</a>)</i>
                        </td>
                        </td>
                        <td><a href="https://drive.google.com/file/d/1C_tnGWHAQOm6YfyhvV2nMtMHtBzmcK64/view?usp=sharing" target="_blank">Lab0</a></td>
                        <td><a href="https://github.com/10605/released-hws/releases/tag/s21-hw1" target="_blank">HW1 Release</a></td>
                    </tr>
		
                    <tr>
                        <td>Feb 8</td>
                        <td>
                        Visualization, PCA (<a href="https://drive.google.com/file/d/1zIqeY47m0OHIMQW9zVC7yf-AxMgYXVV2/view?usp=sharing" target="_blank">slides</a>, <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=bd55f4ce-a2f4-4b4f-87fe-acc900fb24bd" target="_blank">video</a>)
                        </td>
                        <td><a href="https://arxiv.org/pdf/1404.1100.pdf" target="_blank">Tutorial on PCA</a><br /><a href="https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf" target="_blank">JL Theorem</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 10</td>
                        <td>
                        Nonlinear Dimensionality Reduction (<a href="https://drive.google.com/file/d/1AWkM2gFQxlUFtewlvUC36dLJtOlhjqg_/view?usp=sharing" target="_blank">slides</a>)
                        </td>
                        <td><a href="https://lvdmaaten.github.io/tsne/" target="_blank">t-SNE</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Basics of Large-Scale / Distributed Machine Learning</td>
                    </tr>
                    <tr>
                        <td>Feb 15</td>
                        <td>
                        Distributed Linear Regression, part I
                        </td>
                        <td></td>
                        <td>HW2 Released</td>
                    </tr>
                    <tr>
                        <td>Feb 17</td>
                        <td>
                        Distributed Linear Regression, part II
                        </td>
                        <td></td>
                        <td>HW1 Due</td>
                    </tr>
                    <tr>
                        <td>Feb 22</td>
                        <td>
                        Kernel Approximations
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Feb 24</td>
                        <td>Guest Lecture <a href="http://pages.cs.wisc.edu/~thodrek/" target = "_blank"> Theodoros (Theo) Rekatsinas </a>
                        </td>
                        <td></td>
			<td><b>Scheduled at 11:00am instead of 8:20am</b></td>
                    </tr>
                    <tr>
                        <td>Mar 1</td>
                        <td>
                        Logistic Regression, Hashing
                        </td>
                        <td><a href="https://jmlr.csail.mit.edu/papers/volume10/shi09a/shi09a.pdf" target="_blank">Hash kernels, I</a><br /><a href="https://arxiv.org/pdf/0902.2206.pdf" target="_blank">Hash kernels, II</a></td>
                        <td>HW2 Due, HW3 Released</td>
                    </tr>
                    <tr>
                        <td>Mar 3</td>
                        <td>
                        Randomized Algorithms
                        </td>
                        <td><a href="http://dimacs.rutgers.edu/~graham/pubs/papers/cm-full.pdf" target="_blank">Count-min sketch</a><br /><a href="http://people.csail.mit.edu/indyk/p117-andoni.pdf" target="_blank">LSH</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 8</td>
                        <td>
                        Distributed Trees
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 10</td>
                        <td> TBA
                        </td>
                        <td></td>
                        <td>HW3 Due</td>
                    </tr>
                    <tr>
                        <td>Mar 15</td>
                        <td>
                        <b><i>Exam I</i></b>
                        </td>
                        <td></td>
                        <td>HW4 Released</td>
                    </tr>
                    <tr>
                        <td>Mar 17</td>
                        <td>Cloud Computing &#38 Services
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Scalable Deep Learning: Training, Tuning, and Inference</td>
                    </tr>
                    <tr>
                        <td>Mar 22</td>
                        <td>Deep Learning, Autodiff
                        </td>
                        <td><a href="https://www.deeplearningbook.org/contents/mlp.html" target="_blank">Deep Learning, Ch. 6</a><br /><a href="https://www.tensorflow.org/tutorials/quickstart/beginner" target="_blank">TensorFlow Quickstart</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 25</td>
                        <td>
                        DL Frameworks, Hardware
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Mar 29</td>
                        <td>
                        Large-Scale Optimization
                        </td>
                        <td><a href="https://arxiv.org/abs/1606.04838" target="_blank">Optimization for Large-Scale ML</a></td>
                        <td>HW4 Due, HW5 Released</td>
                    </tr>
                    <tr>
                        <td>Mar 31</td>
                        <td>
                        Optimization for DL
                        </td>
                        <td><a href="https://www.deeplearningbook.org/contents/optimization.html" target="_blank">Deep Learning, Ch. 8</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 5</td>
                        <td>
                        <b>No Class (CMU Break Day)</b>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 7</td>
                        <td>
                        Parallel/Distributed DL
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 12</td>
                        <td>
                        Hyperparameter Tuning
                        </td>
		        <td><a href="https://homes.cs.washington.edu/~jamieson/hyperband.html" target="_blank">blog1</a>, <a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank">blog2</a></td>
                        <td>HW5 Due, HW6 Released</td>
                    </tr>
                    <tr>
                        <td>Apr 14</td>
                        <td>
                        Inference, Model Compression
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td colspan=4, style="text-align: center; background-color: #e6f5ff">Advanced Topics</td>
                    </tr>
                    <tr>
                        <td>Apr 19</td>
                        <td>
                        Neural Architecture Search
                        </td>
                        <td><a href="https://blog.ml.cmu.edu/2020/07/17/in-defense-of-weight-sharing-for-nas/" target="_blank">blog</a>, <a href="https://arxiv.org/abs/1902.07638" target="_blank">RSWS</a></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 21</td>
                        <td> Guest Lecture
                        </td>
                        <td></td>
                        <td>HW6 Due, HW7 Released</td>
                    </tr>
                    <tr>
                        <td>Apr 26</td>
                        <td>
                        Productionizing Large-Scale ML
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Apr 28</td>
                        <td>
                        Federated Learning
                        </td>
                        <td></td>
                        <td>HW7 Due</td>
                    </tr>
                    <tr>
                        <td>May 3</td>
                        <td>
                        Course Summary
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>May 5</td>
                        <td>
                        <b><i>Exam II</i></b>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                </tbody>
            </table>
        </div>
		
        <br /><br />
		<h3 id="policies">General Policies</h3>
		<h4>Late Homework Policy</h4>
		You receive 4 total grace days for use on any homework assignment. We will automatically keep a tally of these grace days for you; they will be applied greedily. No assignment will be accepted more than 1 days after the deadline without written permission from Daniel, Ameet, or Virginia. You may not use more than 1 grace day on any single assignment.<br /> <br />All homework submissions are electronic (see Technologies section below). As such, lateness will be determined by the latest timestamp of any part of your submission. For example, suppose the homework requires submissions to both Gradescope Written and Programming– if you submit your Written on time but your Programming 1 minute late, your entire homework will be penalized for the full 24-hour period.
		
		<h4>Extensions</h4>
		In general, we do not grant extensions on assignments. There are several exceptions:
		<ul>
		<li>Medical Emergencies: If you are sick and unable to complete an assignment or attend class, please go to University Health Services. For minor illnesses, we expect grace days or our late penalties to provide sufficient accommodation. For medical emergencies (e.g. prolonged hospitalization), students may request an extension afterwards and should include a note from University Health Services.</li>
		<li>Family/Personal Emergencies: If you have a family emergency (e.g. death in the family) or a personal emergency (e.g. mental health crisis), please contact your academic adviser or Counseling and Psychological Services (CaPS). In addition to offering support, they will reach out to the instructors for all your courses on your behalf to request an extension.</li>
		<li>University-Approved Absences: If you are attending an out-of-town university approved event (e.g. multi-day athletic/academic trip organized by the university), you may request an extension for the duration of the trip. You must provide confirmation of your attendance, usually from a faculty or staff organizer of the event.</li>
		</ul>
For any of the above situations, you may request an extension by emailing Daniel Bird (dpbird@andrew.cmu.edu). The email should be sent as soon as you are aware of the conflict and at least 5 days prior to the deadline. In the case of an emergency, no notice is needed.
		
		<h4>Audit Policy</h4>
		Official auditing of the course (i.e. taking the course for an “Audit” grade) is not permitted this semester.<br /> <br />Unofficial auditing of the course (i.e. watching the lectures online or attending them in person) is welcome and permitted without prior approval. Unofficial auditors will not be given access to course materials such as homework assignments and exams.

		<h4>Pass/Fail Policy</h4>
		Pass/Fail is only allowed in this class with written permission from Ameet or Virginia. The grade for the Pass cutoff will depend on your program. Be sure to check with your program / department as to whether you can count a Pass/Fail course towards your degree requirements.
		
		<h4>Accommodations for Students with Disabilities </h4>
		If you have a disability and have an accommodations letter from the Disability Resources office, I encourage you to discuss your accommodations and needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them at <a href= "mailto:access@andrew.cmu.edu">access@andrew.cmu.edu</a>.
		
		<h3>Academic Integrity Policies</h3>
		<b>Read this Carefully</b>
		<h4>Collaboration among Students</h4>
		<ul>
		<li>The purpose of student collaboration is to facilitate learning, not to circumvent it. Studying the material in groups is strongly encouraged. It is also allowed to seek help from other students in understanding the material needed to solve a particular homework problem, provided no written notes (including code) are shared, or are taken at that time, and provided learning is facilitated, not circumvented. The actual solution must be done by each student alone.</li>
		<li>The presence or absence of any form of help or collaboration, whether given or received, must be explicitly stated and disclosed in full by all involved. Specifically, each assignment solution must include the corresponding collaboration section.</li>
		<li>If you gave help after turning in your own assignment and/or after answering the collaboration section, you must update your answers before the assignment’s deadline, if necessary by emailing the course staff or a Piazza post.</li>
		<li>Collaboration without full disclosure will be handled severely, in compliance with CMU’s Policy on <a href= "https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html" target = "_blank">Academic Integrity.</a></li>
		</ul>
		
		<h4>Previously Used Assignments</h4>
		Some of the homework assignments used in this class may have been used in prior versions of this class, or in classes at other institutions, or elsewhere. Solutions to them may be, or may have been, available online, or from other people or sources. It is explicitly forbidden to use any such sources, or to consult people who have solved these problems before. It is explicitly forbidden to search for these problems or their solutions on the internet. You must solve the homework assignments completely on your own. We will be actively monitoring your compliance. Collaboration with other students who are currently taking the class is allowed, but only under the conditions stated above.
		
		<h4>Policy Regarding “Found Code”</h4>
		You are encouraged to read books and other instructional materials, both online and offline, to help you understand the concepts and algorithms taught in class. These materials may contain example code or pseudo code, which may help you better understand an algorithm or an implementation detail. However, when you implement your own solution to an assignment, you must put all materials aside, and write your code completely on your own, starting “from scratch”. Specifically, you may not use any code you found or came across. If you find or come across code that implements any part of your assignment, you must disclose this fact in your collaboration statement.
		
		<h4>Duty to Protect One’s Work</h4>
		Students are responsible for proactively protecting their work from copying and misuse by other students. If a student’s work is copied by another student, the original author is also considered to be at fault and in gross violation of the course policies. It does not matter whether the author allowed the work to be copied or was merely negligent in preventing it from being copied. When overlapping work is submitted by different students, both students will be punished.<br /> <br />To protect future students, do not post your solutions publicly, neither during the course nor afterwards.

		<h4>Penalties for Violations of Course Policies</h4>
		All violations (even first one) of course policies will always be reported to the university authorities (your Department Head, Associate Dean, Dean of Student Affairs, etc.) as an official Academic Integrity Violation and will carry severe penalties.
		<ol>
		<li>The penalty for the first violation is a one-and-a-half letter grade reduction. For example, if your final letter grade for the course was to be an A-, it would become a C+.</li>
		<li>The penalty for the second violation is failure in the course, and can even lead to dismissal from the university.</li>
		</ol>

		
		<br /><br />
		<h3>Acknowledgments</h3>
        <p> This course is based in part on material developed by <a href="https://heather.miller.am/" target="_blank">Heather Miller</a>, <a href="http://www.cs.cmu.edu/~wcohen/" target="_blank">William Cohen</a>, <a href="http://www.cs.berkeley.edu/~adj/" target="_blank">Anthony Joseph</a>, and <a href="http://www.cs.cmu.edu/~bapoczos/" target="_blank">Barnabas Poczos</a>.</p>
        <p>Previous courses: <a href="https://10605.github.io/fall2020/index.html" target="_blank">10-605/10-805, Fall 2020</a>; <a href="https://10605.github.io/spring2020/index.html" target="_blank">10-405/10-605, Spring 2020</a>.</p>
        <hr>
		
		
		
		
		
		
        </div>
		
		

    </div>
    <!-- /container 


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>
    <script>
      anchors.options.visible = 'always';
      anchors.add('.anchored');
    </script>


</body>
