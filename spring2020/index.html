<head>
    <meta charset="utf-8">
<title> CMU 10-405/10-605 </title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../docs-assets/ico/favicon.png">

    <!-- Bootstrap core CSS -->
    <link href="bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
</head>

<body>
    <div class="container">
	 <div class="panel-footer clearfix">
        <div class="pull-left">

            <h1>10-405/10-605: ML with Large Datasets, Spring 2020</h1>
            <p class="lead">Instructors:</p><p><a href=https://heather.miller.am/ target="_blank">Prof. Heather Miller</a> (OH: Fri 11am-12pm)<br /><a href=https://www.cs.cmu.edu/~smithv/ target="_blank">Prof. Virginia Smith</a> (OH: Wed 2pm-3pm)</a></p>
            <p class="lead">Lecture:</p>
            <p>Mon/Wed 9:00am-10:20am, HOA 160
            <!--<br/>Instructor office hours are held right after lectures.-->
            </p>
            <p class="lead">Recitation:</p>
            <p>Fri 9:00am-10:20am, HOA 160
            <!--<p class="lead"> TA Office Hours: </p>
            <br/>Monday 9.30-10.30AM ET at Gates 3rd floor cafe (), Wednesday 5-6PM ET at REH 351 (), Friday 10-11AM ET at HH1209 ()</p>-->
            </div>
        </div>

        <div class="main">
        <br>
        <h3>Course Overview</h3>
        <p class="p1">Large datasets pose difficulties across the machine learning pipeline. They are difficult to visualize, and it can be hard to determine what sorts of errors and biases may be present in them. They are computationally expensive to process, and the cost of learning is often hard to predict---for instance, an algorithm that runs quickly on a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.<br /><br />This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the topics considered are: data cleaning, visualization, and pre-processing at scale; principles of parallel and distributed computing for machine learning; techniques for scalable deep learning; analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity; and methods for low-latency inference. Students will gain experience with common large-scale computing libraries and infrastructure, including Apache Spark and TensorFlow.
        </p>

        <h3>Prerequisites</h3>
        <p>Students are required to have taken a CMU introductory machine learning course (10-401, 10-601, 10-701, or 10-715). A strong background in programming will also be necessary; suggested prerequisites include 15-210, 15-214, or equivalent. Students are expected to be familiar with Python or learn it during the course.</p>

        <h3>Textbooks</h3>
        There will be no required textbooks, though we may suggest additional reading in the schedule below.

        <h3>Piazza</h3>
        <p> We will use Piazza for class discussions. Please go to <a href=http://piazza.com/cmu/spring2020/1040510605 target="_blank" >this Piazza website</a> to join the course forum (note: you must use a cmu.edu email account to join the forum). We strongly encourage students to post on this forum rather than emailing the course staff directly (this will be more efficient for both students and staff). Students should use Piazza to:
        <ul>
          <li>Ask clarifying questions about the course material.</li>
          <li>Share useful resources with classmates (so long as they do not
          contain homework solutions).</li>
          <li>Look for students to form study groups.</li>
          <li>Answer questions posted by other students to solidify your own
          understanding of the material.</li>
        </ul>
        The course Academic Integrity Policy must be followed on the message boards at all times. <b>Do not post or request homework solutions!</b> Also, please be polite.

        <h3>Course Staff</h3>


        <p><b>Teaching Assistants</b></p>

        <div class="row">
          <div class="col-md-3">
            <center>
              <img src="resources/img/kushagr.png">
              <p>
                <b>Kushagr Arora</b>
                <br/>OH: Thursdays 3-4pm
              </p>
            </center>
          </div>
          <div class="col-md-3">
            <center>
              <img src="resources/img/saket.png">
              <p>
                <b>Saket Chaudhary</b>
                <br/>OH: Fridays 3-4pm
              </p>
            </center>
          </div>
          <div class="col-md-3">
            <center>
              <img src="resources/img/jiayong.png">
              <p>
                <b>Jiayong Hu</b>
                <br/>OH: Tuesdays 1:30-2:30pm
              </p>
            </center>
          </div>
          <div class="col-md-3">
            <center>
              <img src="resources/img/anwen.png">
              <p>
                <b>Anwen Huang</b>
                <br/>OH: Mondays 2-4pm
              </p>
            </center>
          </div>
        </div>

        <div class="row">
          <div class="col-md-3">
            <center>
              <img src="resources/img/tian.png">
              <p>
                <b>Tian Li</b>
                <br/>OH: Wednesdays 11am-12pm
              </p>
            </center>
          </div>
          <div class="col-md-3">
            <center>
              <img src="resources/img/daniel.png">
              <p>
                <b>Daniel Mo</b>
                <br/>OH: Mondays 4-6pm
              </p>
            </center>
          </div>
          <div class="col-md-3">
            <center>
              <img src="resources/img/zach.png">
              <p>
                <b>Zach (Zeyu) Peng</b>
                <br/>OH: Mondays 12-2pm
              </p>
            </center>
          </div>
          <div class="col-md-3">
            <center>
              <img src="resources/img/kuo.png">
              <p>
                <b>Kuo Tian</b>
                <br/>OH: Fridays 1-3pm
              </p>
            </center>
          </div>
        </div>


        <h3>Grading Policy</h3>
        <p> Grades will be based on the following components:
        <ul>
          <li> <b>Assignments (25%)</b>: There will be 5 homework assignments. Each
          each assignment will have equal weight.
          <ul>
            <li><b>Late submissions will not be accepted.</b>
            <li> <i>There is one exception to this rule:</i> You are given 2 "late days" (self-granted 24-hr extensions) which you can use to give yourself extra time without penalty. <b>At most one late day can be used per assignment.</b>
            <li>There is one TA responsible for each assignment, as indicated in the schedule below. <i>Direct all communication regarding the assignment to this TA.</i></li>
          </ul>
          <li> <b>Midterm (20%) and Final (25%)</b>: These in-person exams will cover material
          from the lectures and assignments.
          <li><b>Project (25%)</b>: The project is an opportunity to get hands-on experience applying machine learning at scale. We will not consider projects that can easily be executed on a laptop.</li>
          <ul>
              <li>You must work in teams of 4-6 people.</li>
              <li>There will be two deliverables: a project proposal and project report.</li>
              <li><i>Additional details to follow.</i></li>
          </ul>
          <li> <b>Class Participation (5%)</b>: Participation will be recorded via in-class quizzes that will be carried out in most classes. To get full credit for class participation you need to attend at least 80% of the lectures based on the polls we conduct.</li>
          <li> <b>Bonus</b>: On Piazza, the top student “endorsed answer” answerers can earn bonus points.</li>
        </ul>

        <h3>Academic Integrity Policy</h3>
        Group studying and collaborating on problem sets are encouraged;
        working together is a great way to understand new material.  Students
        are free to discuss the homework problems with anyone under the
        following conditions:
        <ul>
          <li>Students must submit their own homework solutions and understand the
          solutions that they submit.</li>
          <li>Students must list the names of their collaborators (i.e., anyone
          with whom the assignment was discussed).</li>
          <li>Students may not use old homework solutions from other classes under any circumstances, unless the instructor grants
          special permission.</li>
        </ul>

        Students are encouraged to read CMU's <a
          href=https://www.cmu.edu/policies/
          target="_blank"> Policy on Cheating and Plagiarism</a>.

        <h3>A Note on Self Care</h3>
        <p>Please take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep, and taking some time to relax. This will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. You are not alone. Besides the instructors, who are here to help you succeed, there are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is often helpful.</p>

        <p>If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website at <a href="https://www.cmu.edu/counseling/">https://www.cmu.edu/counseling/</a>. Consider reaching out to a friend, faculty, or family member you trust for help getting connected to the support that can help.</p>

        <h3>Acknowledgments</h3>
        <p> This course is based in part on material developed by <a href="http://www.cs.cmu.edu/~wcohen/" target="_blank">William Cohen</a>, <a href="http://www.cs.cmu.edu/~bapoczos/" target="_blank">Barnabas Poczos</a>, <a href="https://www.cs.cmu.edu/~atalwalk/" target="_blank">Ameet Talwalkar</a>,  and <a href="http://www.cs.berkeley.edu/~adj/" target="_blank">Anthony Joseph</a>.
        <hr>


        <h3>Schedule (Subject to Change)</h3>
        <div class="bs-example">
            <table class="table">
                <thead>
                    <tr>
                        <th>Date</th>
                        <th>Topics</th>
                        <th>Resources</th>
                        <th>HW</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="active">
                        <td>1/13</td>
                        <td><a href="https://drive.google.com/open?id=1OCuW3fGJiYiWGI124KrZw1aFsJb3LSjR" target="_blank">Introduction</a>
                        </td>
                        <td><a href="https://arxiv.org/pdf/1904.03257.pdf" target="_blank">MLSys: The New Frontier of ML Systems</a></td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>1/15</td>
                        <td><a href="https://drive.google.com/file/d/1PD9ySy1XSbm8gC0aDX59P0cOO_mwx8Eu/view?usp=sharing" target="_blank">Distributed Computing, MapReduce</a>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="warning">
                        <td>1/17</td>
                        <td><i>Recitation: Spark toplogy basics + setup with Databricks</i><br />(<a href="https://docs.google.com/presentation/d/1ucvv2a19xG_maOwCd6ZPQuhEaWCqJlxEbBchJfcuVbc/edit?usp=sharing" target="_blank">Slides (Tian)</a>, <a href="https://drive.google.com/file/d/16XM23DC1i1pT6Nt3Vl-23QMKIwhjaiy_/view?usp=sharing">Slides (Heather)</a>, <a href="https://drive.google.com/file/d/1IW69-crS6ebCh3bANAMWfatuzRaytKiN/view?usp=sharing" target="_blank">Lab0</a>)</a>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>1/20</td>
                        <td><b>No class (MLK Day)</b>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>1/22</td>
                        <td><a href="https://drive.google.com/open?id=13b_bzIFuQ1fZDodGI8_ZOlE4NqRqbsbV">Intro to Spark</a>
                        </td>
                        <td></td>
                        <td><a href="https://github.com/10605/released-hws/blob/master/hw1/hw1.pdf">HW1</a> released</td>
                    </tr>
                    <tr class="warning">
                        <td>1/24</td>
                        <td><i>Recitation: Spark Transformations and Actions</i><br />(<a href="https://drive.google.com/file/d/1LNOa8IlgxmFkvXl6zGxoThxJoWa0VP06/view?usp=sharing" target="_blank">Lab2 (Notebook)</a>, <a href="https://drive.google.com/file/d/1SDXaALpPXAi6CMNYcupEloc-FvYJxmm9/view?usp=sharing">Slides</a>)
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>1/27</td>
                        <td><a href="https://drive.google.com/open?id=1jU7EBsWbaO_p8ag8viP5m74rFTiqwdIE" target="_blank">Data Cleaning</a><br /><a href="https://drive.google.com/open?id=1B5ZtlJdI7hc4ubi3tTUf-gF7ZBCxzfvF" target="_blank">Spark: Joins, Structure, and DataFrames</a>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>1/29</td>
                        <td><a href="https://drive.google.com/file/d/1Hia9PVtwqmSfDnbQdeoz4hZkKEwODQTM/view?usp=sharing" target="_blank">Data Visualization</a>
                        </td>
                        <td><a href="https://www.youtube.com/watch?v=ze08gwVPaXk" target="_blank">Visualization for ML</a><br /><a href="https://arxiv.org/pdf/1404.1100.pdf" target="_blank">A Tutorial on PCA</a><br /><a href="https://lvdmaaten.github.io/tsne/" target="_blank">t-SNE</a></td>
                        <td></td>
                    </tr>
                    <tr class="warning">
                        <td>1/31</td>
                        <td><i>Recitation: Spark RDDs and DataFrames</i><br />(<a href="https://drive.google.com/open?id=1zKx2ekTAdS8PqCqppCudH_5YJvflifZU" target="_blank">Lab3 (Notebook)</a>, Slides are those from Monday's lecture)
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>2/3</td>
                        <td><a href="https://drive.google.com/file/d/144qs1qXFw5h4l3GzB4cwgVLdWKSIQZmB/view?usp=sharing" target="_blank">ML Review</a>
                        </td>
                        <td><a href="https://www.deeplearningbook.org/contents/ml.html" target="_blank">Deep Learning, Ch. 5.2-5.4</a><br /><a href="http://gwthomas.github.io/docs/math4ml.pdf" target="_blank">Math for ML (review)</a></td>
                        <td>HW1 due</td>
                    </tr>
                    <tr class="success">
                        <td>2/5</td>
                        <td><a href="https://drive.google.com/open?id=1yREU5piZCd13uzou5gQmsq7fpf4eQjuT" target="_blank">Distributed Linear Regression, Part I</a>
                        </td>
                        <td></td>
                        <td><a href="https://github.com/10605/released-hws/blob/master/hw2/hw2.pdf">HW2</a> released</td>
                    </tr>
                    <tr class="warning">
                        <td>2/7</td>
                        <td><i>Recitation: Linear Algebra Review</i><br />(<a href="https://github.com/10605/Recitation/blob/master/02_07_linear-algebra.pdf" target="_blank">Slides</a>)
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>2/10</td>
                        <td><a href="https://drive.google.com/open?id=1yREU5piZCd13uzou5gQmsq7fpf4eQjuT" target="_blank">Distributed Linear Regression, Part II</a>
                        </td>
                        <td></td>
                        <td></td>
                    <tr class="success">
                        <td>2/12</td>
                        <td><a href="https://drive.google.com/file/d/1zgX6OtqnhXsVi0ftL0PwZUFfYq7C1Axs/view?usp=sharing" target="_blank">Adv. Distributed Optimization</a>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="warning">
                        <td>2/14</td>
                        <td><i>Recitation: Learning Rate Optimization</i><br />(<a href="https://github.com/10605/Recitation/blob/master/recitation_4.ipynb" target="_blank">Lab4 (Notebook)</a>, <a href="https://github.com/10605/Recitation/blob/master/02_14_recitation_4.pdf">Slides</a>)
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>2/17</td>
                        <td><a href="https://drive.google.com/file/d/15RnIz1HeQU2NDLXDmA2dBEDIRB5PJDM5/view?usp=sharing" target="_blank">Distributed Logistic Regression</a>
                        </td>
                        <td></td>
                        <td>HW2 due</td>
                    </tr>
                    <tr class="success">
                        <td>2/19</td>
                        <td><a href="https://drive.google.com/file/d/12xOq6e1bxtbmnjdTnG5-17866OaM621c/view?usp=sharing">Partitioning and Locality</a>
                        </td>
                        <td>
                        </td>
                        <td><a href="https://github.com/10605/released-hws/blob/master/hw3/hw3.pdf">HW3</a> released</td>
                    </tr>
                    <tr class="warning">
                        <td>2/21</td>
                        <td><i>Recitation: Probability Review</i><br />(<a target="_blank" href="https://github.com/10605/Recitation/blob/master/02_21_prob_review.pdf">Slides</a>)
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>2/24</td>
			                   <td><a href="https://drive.google.com/file/d/1zRyVFZmYkqtg_IBCbr4m2eJ32poUBK0L/view?usp=sharing" target="_blank">Large-Scale Data Structures</a>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>2/26</td>
                        <td><a href="https://drive.google.com/open?id=1f1DA7gvmN4ySS3qiD1z4S4awQB07I_mj" target="_blank">PCA</a>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>3/2</td>
                        <td>Project Proposals
                        </td>
                        <td>
                        </td>
                        <td>HW3 due</td>
                    </tr>
                    <tr class="success">
                        <td>3/4</td>
                        <td><b><i>In-Class Midterm</i></b>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>3/9</td>
                        <td><b>No class (Spring Break)</b>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                     <tr class="success">
                        <td>3/11</td>
                        <td><b>No class (Spring Break)</b>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>3/16</td>
                        <td><b><i>All CMU classes cancelled</i></b>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>3/18</td>
                        <td><a href="https://drive.google.com/file/d/1knjTbvb5rQkkVexDW_Ypmq7cytt07p62/view?usp=sharing" target="_blank">Deep Learning</a>
                        </td>
                        <td><a href="https://www.deeplearningbook.org/contents/mlp.html" target="_blank">Deep Learning, Ch. 6</a><br /><a href="http://www.cs.cmu.edu/~wcohen/10-605/notes/autodiff.pdf" target="_blank">William Cohen's Autodiff Notes</a>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>3/23</td>
                        <td><a href="https://drive.google.com/open?id=1tqrByBD2k0ruHpQ4aWBwu9yikDaLY3vw">ML Frameworks + TensorFlow</a>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>3/25</td>
                        <td><a href="https://drive.google.com/open?id=1bvbwSun1GTQBHFhkubz4vD6tz8EQk2Kc">ML Hardware + TensorFlow</a>
                        </td>
                        <td>
                          <a href="https://www.youtube.com/watch?v=yH1cF7GnoIo">Performant, scalable models in TensorFlow 2 with tf.data, tf.function & tf.distribute (TF World '19) [Video]</a>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>3/30</td>
                        <td><a href="https://drive.google.com/file/d/1PA0ARElkcUZYuu4fPk_AbrFKF1fwsfI-/view?usp=sharing" target="_blank">Optimization for DL</a>
                        </td>
                        <td><a href="https://www.deeplearningbook.org/contents/optimization.html" target="_blank">Deep Learning, Ch. 8</a>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>4/1</td>
                        <td><a href="https://drive.google.com/open?id=1t9vW84Lnt9Z2oS_q4YjIQv3u3FZ2UOGl" target="_blank">Efficient Hyperparameter Tuning</a><br /><i>Guest lecture: <a href="https://liamcli.com/" target="_blank">Liam Li</a></i>
                        </td>
                        <td><a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank">MLD blog post</a>
                        </td>
                        <td><a href="https://github.com/10605/released-hws/blob/master/hw4/hw4.pdf">HW4</a> released</td>
                    </tr>
                    <tr class="active">
                        <td>4/6</td>
                        <td><a href="https://drive.google.com/file/d/1KHYMQo4pFbH8Rv0cNAZhwKHOoUwZzMO3/view?usp=sharing" target="_blank">Parallel/Distributed DL</a>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>4/8</td>
                        <td><a href="https://drive.google.com/file/d/1wB8R2KzTI0DeBTauzJugpBGHiQR_1DOK/view?usp=sharing" target="_blank">Inference and Model Compression</a>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="warning">
                        <td>4/10</td>
                        <td><i>Project check-ins</i>
                        </td>
                        <td></td>
                        <td>HW4 due,<br /><a href="https://github.com/10605/released-hws/blob/master/hw5/hw5.pdf">HW5</a> released</td>
                    </tr>
                    <tr class="active">
                        <td>4/13</td>
                        <td><a href="https://drive.google.com/file/d/14y8KzWBx-q3pyS_T-wmm2W1Ssb-RRCuh/view?usp=sharing" target="blank">TVM & DL Compilers</a><br/><i>Guest lecture: <a href="https://tqchen.com/" target="_blankl">Tianqi Chen</a></i>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>4/15</td>
                        <td><a href="https://drive.google.com/file/d/1wKsP_9LwC0tMR1_9O9X8irZCXlGxsE8l/view?usp=sharing">Productionizing Large-Scale ML</a>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>4/20</td>
                        <td><a href="https://drive.google.com/file/d/1fPPoG0geY87C8x43-G-QF1pVKLM1CMs5/view?usp=sharing" target="_blank">Federated Learning</a>
                        </td>
                        <td>
                        </td>
                        <td>HW5 due</td>
                    </tr>
                    <tr class="success">
                        <td>4/22</td>
                        <td><b><i>Project Presentations I</i></b><br />
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="warning">
                        <td>4/24</td>
                        <td><b><i>Project Presentations II</i></b><br />
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="active">
                        <td>4/27</td>
                        <td><a href="https://drive.google.com/file/d/123WIRnRIujy61VcnroH8XY--chjFw_0T/view?usp=sharing" target="_blank">Course summary</a>
                        </td>
                        <td>
                        </td>
                        <td></td>
                    </tr>
                    <tr class="success">
                        <td>4/29</td>
                        <td><b><i>In-Class Final</i></b>
                        </td>
                        <td></td>
                        <td></td>
                    </tr>
                    </tr>
                </tbody>
            </table>
        </div>
        </div>

        <hr>


      <h2 class="anchored">Completed Projects from Spring 2020</h2>
      <p>&nbsp;</p><p>&nbsp;</p>


      <!-- start group -->
      <h3 class="anchored">DeepGenre: Deep Neural Networks for Genre Classification in Literary Works</h3>
      <p>
        <b>Zihan Huang, Yikang Li, Scott Liu, Zhanlin Sun, Jiahao Wu, Hongyi Zhang</b>
      </p>
      <p>
        In this project, we attempt to address a multi-label text classification problem that predominately features very long text-based inputs. In particular, we focus on using the Gutenberg Project dataset and use the main text of an e-book to infer its genre. This is motivated partly because many e-books, especially new ones, may have few or no labeled genres; an automated approach would help curators and librarians assign correct genres for better cataloguing of library resources. We propose to use feature engineering combined with a distributed approach using deep neural networks to tackle long textual inputs. We evaluate and benchmark various models according to our custom metrics in order to determine their effectiveness.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1dskHpWp1-qycwr6SiAhLlswHnpne9zCE/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Predicting Hotness from Million Song Dataset</h3>
      <p>
        <b>Andrew Alini, Christopher	Benson, Harsh Jain, Sabyasachi	Mohanty, Varun Natu</b>
      </p>
      <p>
        The pace of music production is ever increasing, so it can be hard for those in the music industry to consume and rate new music as it is released. To address this, we created a system that rates the “hotness” of a new unseen song using the MCFF audio features of a song. We trained on the Million Song Dataset, and took advantage of distributed ML frameworks such as SparkML, Tensorflow, and Horovod.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1BsRhNhAdwKrqucgEM1ivgD8ZKE6VKhwE/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Automated Road Network Extraction and Route Travel Time Estimation from Satellite Imagery</h3>
      <p>
        <b>Phalguna	Dasaratha Mankar, Karan Vasant Hebbar, Sameed Qureshi, Vedant Sanil, Zeeshan Ashraf Shaikh, Sharath Srikanth Chellappa</b>
      </p>
      <p>
        We focused on data from the SpaceNet Challenge. The aim of this challenge is to build models at scale that are able to use satellite imagery to not only detect the network of roads but also provide an estimate of the travel time along the different routes. With this data, we aimed to answer the following questions: (1) Is it possible to use satellite images to accurately identify roads? (2) Is it possible to build a network (graph structure) from the identified roads? (3) Is it possible to estimate travel times using the graph network (on the detected edges)?
      </p>
      <p>
        The data that we had was first divided into the image data and the metadata. The image data consisted of approximately 2500 annotated images in total for training, and approximately 930 test images. The images were present in TIF format. The metadata consisted of GeoJSON data, linestring data (road graphs), and TIF geodata images. For the GeoTiff processing we used the GDAL and CV2 package.
      </p>
      <p>
        With this processed data, the end goal of our model was to be able to segment out roads from the satellite images and predict travel times for the roads. The processed 8-bit image was first fed into 4 separately trained UNet-inspired models in parallel. The models have a ResNet34 encoder and a U-Net decoder. These models output the segmentation masks of the roads. For robustness, the output from the four models was superimposed to create a final segmentation output. The segmented image was then smoothed out, small gaps were closed out, spurious connections were removed. An attempt was also made to clean hanging edges and connect terminal vertices near non-connected nodes. From the final cleaned segmented image, we extracted the skeleton using skimage. From the skeleton the graph was extracted using sknw - a python library to convert a skeleton to a graph.
      </p>
      <p>
        We stored the data on AWS s3 bucket and accessed this in the ec2 instance created. We used AWS g4xdn.large ec2 instance and GCP VM instances to train the model for 50 epochs which took around 3-4 hours to train. With this, we were able to accurately identify the roads from the satellite images and build a road network from the identified roads.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1z0HHJEuwKT0zPdMG-kr_7jaZMyWFrRco/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Exploring Relationships Between Subreddits</h3>
      <p>
        <b>Linhong Li, Simin Li, Xinwen Liu, Junyan Pu</b>
      </p>
      <p>
        Reddit is a forum where people can comment on many different topics organized by subreddits. The question we set out to answer is: What is the relationships between different subreddits across time and how do we interpret them? For this project, our methods included TF-IDF for tokenization, LDA topic modeling and PCA for dimensionality reduction, and t-SNE and K-means clustering for evaluation.  We used Dataproc on Google Cloud Platform (GCP) as our cluster service provider and a 403 GB public Reddit dataset available via GCP’s BigQuery platform. We found that 1. The size of clusters and membership of subreddits change over time and 2. LDA outperforms PCA in terms of interpretability but underperforms in terms of silhouette score.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1Qk_MRTr_WJHqZdOpVMDg0R2JrUtDScOn/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Predicting the Stock Market with Reddit Comments</h3>
      <p>
        <b>Zilin Jiang, Ziqian Luo, Ruobing Wang, Fangzhou Xie, Anxiang Zhang</b>
      </p>
      <p>
        The question we wanted to answer was whether we could predict the stock market trends using Reddit comments. We used sentiment analysis and PCA to preprocess the data and used logistic regression and hyperparameter searching to acquire the best performing model. We used PySpark to train our models on AWS EMR machines and AWS S3 bucket to store datasets. Eventually, our best performing model achieved around 65% accuracy on the testing dataset. One important lesson we have learned is that large scale machine learning tasks can be time-consuming in terms of both implementation and training. So it is the best to design and plan carefully ahead of time.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1WflM8lesxF8tf8P6EnpqHIhbpI-yaPLi/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Year and Decade Prediction on Million Song with Spark</h3>
      <p>
        <b>Xinnan Du, Shengrui Lyu, Xuewei Wang, Xiao Zeng, Zihang Zhang, Junpei Zhou</b>
      </p>
      <p>
        We try to predict the year and decade of a song based on its timbre feature on the Million Song dataset. We explore different dimension reduction techniques (PCA and t-SNE) and different distributed ML models including Logistic Regression, Naive Bayes, and Random Forest. We use Spark with MLlib to build our pipeline and run the distributed model on AWS EMR. We find that the decade pattern is more clear than the year pattern from the visualization result, which is consistent with the experimental result that predicting the decade is much easier. Another interesting observation is that for the difficult setting (predicting the year) the Random Forest performs much better than other methods (also cost much more to train), but for the easy setting (predicting the decade) the gap between models is smaller, where the light-weight model is preferred.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1isGNi4uNuMaTQydbdYNYWJ2qMDfjtkly/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Comparison of dimension reduction algorithms on Million Song Dataset</h3>
      <p>
        <b>Leyi Cai, Zhonglin Cao, Xin Shen, Yijie Zhang, Ping Zhong</b>
      </p>
      <p>
        This project made a comparison of dimension reduction methods using the Million Song dataset. The algorithms compared include PCA, AutoEncoder and LargeVis.The metrics we used include basic ones like running time, memory usage, and model linearity. A regression model was run with dimension reduced data to predict song hotness, the RMSE and Pearson R value of the regression is also used as metrics for algorithms comparison. We discovered that PCA was the most efficient model for this case because it was much faster, took less memory and preserves more information. As a built-in algorithm in pyspark library, PCA is also much easier to use.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1CRXo83vbnJrpqr9tgsIjV4OIaEvEAge_/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Hotness Prediction on Million Song Dataset</h3>
      <p>
        <b>Ziqi Chen, Nan Gao, Jingyi Jiang, Junjie Wang, Yunjia Wang</b>
      </p>
      <p>
        In this project, we are trying to evaluate the performance of multiple models in predicting song hotness based on a large number of features in the Million Song Dataset. We mainly focus on three machine learning models, Linear Regression, Decision Tree regression and Gradient Boosting Regression. We use AWS EBS for data storage and we run spark on EMR instances for distributed training. Our results have shown the pros and cons of each model in terms of accuracy, time, storage, communication costs, east of use and interpretability.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1cvE7hwgaKR0W5yJRR3CdibKHPoCNryXc/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Genre Classification on Million Song Dataset</h3>
      <p>
        <b>Max Bai, Xintong Shi, Yiyi Zou, Beiming Zhang</b>
      </p>
      <p>
        We used machine learning techniques to predict genre label based on songs’ metadata and audio features. We compared the performance of Logistic Regression, Random Forest and Neural Networks on genre classification. Early stage data pre-processing was run on EC2, while later model training and testing was done in pySpark with MLlib running on AWS EMR. As a result, Logistic Regression performed best with cross validation accuracy of 0.9351, while Random Forest produced cross validation accuracy of 0.6018 and Neural Network 0.6934. We have strong reasons to believe that the data might be linearly separable in high dimensions(e.g. separable by high dimensional planes). In such situations, models with linear classification boundaries, such as logistic regression, tend to perform better than those with non-linear classification boundaries(Random Forest, Neural Networks).
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1Fj5XdNa943AxwD14hILhQ0FLtGOHsIRH/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">NIH Chest X-ray Image Classification</h3>
      <p>
        <b>Daniel Chang, Zhiyi Huang, Yichun Li, Yiming Qiu, Yifan Zhou</b>
      </p>
      <p>
        We used NIH Chest X-rays images to make disease predictions. We did both data augmentation including horizontal flip and rotation as well as patient-level dataset split for data preprocessing. We used TensorFlow and compared three CNN models: ResNet50, MobileNetV2, and EfficientNet-B4 for both their accuracy and efficiency. The data preprocessing and training were done in multiple steps on AWS. We found MobileNet to provide both the best accuracy as well as the best efficiency, while EfficientNet which gives state of the art results on ImageNet was not performing as well for our task.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1xbJQvvjjucPyQkqXDzrl5itp8elg2xyT/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Predicting Reddit Score with ML</h3>
      <p>
        <b>George Cai, John Fang, Dennis Li, Yiwen Xu, Zhou Yu, Ting-Yu Lan</b>
      </p>
      <p>
        In this project, we try to predict the score of a Reddit comment as a regression problem given information about that comment and other information in the thread. We applied linear regression, random forests and multi-layer perceptrons to this task. We utilized Microsoft Azure and Google Cloud Platform to process our dataset and run our experiments. We found that random forests had the best performance and achieved a mean average error of only 3.63 points.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/18DupBDJ2-q97zpEnLguO-zrkPeSIWA-J/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">New York Cab Fare Prediction</h3>
      <p>
        <b>Rohit Prakash Barnwal, Bharat Gaind, Abhinav Gupta, Yu-Ning Huang, Anmol Jagetia, Ignacio Maronna Musetti</b>
      </p>
      <p>
        The goal of this project was to efficiently perform fare prediction on NYC cab data, which is 140 GBs. And explore if augmentation with other supplementary datasets provides additional information for the ML model to improve its performance. Our team wrote the code in Spark and PySpark for data cleaning and employed clever tricks to join the diverse datasets. A linear regression model was a good fit for data, and we used the Spark's MLlib library to fit the model. This allowed us to train the model in a distributed environment with 11 instances of m5.xlarge on the AWS EMR cluster. We achieved an RMSE of 3.847 for our prediction model, and provide an interesting analysis of model performance with different fields in the data. The project presented a great hands-on experience with big data and deploying a large scale Machine Learning pipeline.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1Z_jqpHaj10Ad-tEx4MZmr7BSva3yrRKo/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Forecasting Web Page View using Bidirectional LSTM with Auxiliary Features</h3>
      <p>
        <b>Yuqing Deng, Yuzhong Hua, Yuexin Li, Zhongyan Lu, Xinyi Zhang</b>
      </p>
      <p>
        In this project, we explored the possibility for forecasting click trends of webpages using past page view series (~365 GB) from Wikistats PageView Dataset. Aiming to leverage both local contextual information and global periodic patterns, we constructed a machine learning model consisting of three parts- a bidirectional LSTM model, a Dense Layer fed by extracted auxiliary features (such as mean, std, and location of spikes), and an output layer. All raw data is stored on Amazon S3 bucket; data cleaning and processing work are completed on Amazon Elastic Map Reduce (Amazon EMR), using Spark and Python3 for parallel processing; model building, training and validating are completed on EC2, using TensorFlow 2.1.0 and Keras APIs. As a result, our model successfully learned the general curvature of page view of webpages and reduced the average Mean Absolute Error to 10.
      </p>
      <center><iframe src="https://docs.google.com/presentation/d/e/2PACX-1vRDYI-lBji8Wc_I-ipYDMxycv0_D-Rw00MOov_kxVsb3tAb3WAPdA4U_FHOu4cXqMFPj_IHim7eqfri/embed?start=false&loop=false&delayms=3000" frameborder="0" width="624" height="380" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Stack Exchange Answer Ranking and Accepted Answer Prediction</h3>
      <p>
        <b>Bryan Chang, Dahua Gan, Victoria Lin, Mengzhou Xia, Fuya Xu, Endong Zhu</b>
      </p>
      <p>
        In this project, we use machine learning techniques to predict potential accepted answers and to evaluate the quality of answers (answer rankings) on Stack Exchange (Q&A websites). The ML techniques we used include TF-IDF for textual similarity, a base RoBERTa model for contextual representations, random forests, XGBoost with LambdaMART, linear regression, logistic regression, and MLPs. We ran our experiments on GCP and AWS using Spark (PySpark) and a number of Python libraries, including xmltodict, matplotlib, multiprocessing, joblib, pandas, numpy, sklearn, transformers, xgboost4j, matplotlib, seaborn. For the accepted answer prediction task, we achieved a 80.03% question-wise accuracy with BERT, and for the answer ranking task, we achieved 89.76% avgNDCG with XGBoost.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1M5OIulenHm10_qKkSN1pxP8Zkvd1ImFl/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Popularity Prediction for Reddit Comments</h3>
      <p>
        <b>Aditya Anantharaman, Atabak Ashfaq, Manik Bhandari, Preetansh Goyal, Pratik Jayarao, Siddhanth Pillay</b>
      </p>
      <p>
        The project aims to explore the usefulness of NLP features from comment body and additional Reddit features in predicting the popularity of comments. We formulated the problem as a Classification(Popular or not) and Regression problem (upvote prediction) and use Logistic Regression, Random Forests and Linear SVC for the prediction tasks. The project was implemented using Azure and Databricks and we relied on pyspark/MLLib for running distributed processing with python libraries like NLTK for feature engineering. The results show the ability of NLP in predicting comment score while also show scope for improvement which motivates exploring parent and contextual features. One interesting takeaway was that using more and more data beyond a limit did not improve the performance of our simple models like Logistic Regression.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1GvB-Fn0Z2SYuaJqe1nUGJtaKDafIb-Py/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->



      <!-- start group -->
      <h3 class="anchored">Recommender System on Amazon Ratings</h3>
      <p>
        <b>Dipak Krishnan, Vinay Sisodiya Sisodiya, Shuo Wang, Yue Yin, Varun Baldwa, Theodore Li</b>
      </p>
      <p>
        We built a recommender system on the amazon rating dataset containing 233 million reviews. We used collaborative filtering and content-based filtering to train the model. We used Spark and Databricks on the small experimental datasets and did our training with the Amazon EMR cluster, which has 7-10 m5xlarge nodes. Since our dataset's overall rating is skewed, regarding the goal of training a meaningful recommendation system, our model is successful as shown in RMSE and MAE. Large scale data can cause various unexpected problems in ML pipeline.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1N0_0DTZbQCh4D-aDE0FqrckFXGic4bgX/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->



      <!-- start group -->
      <h3 class="anchored">Million Song Dataset: Year and Culture</h3>
      <p>
        <b>Doris Duan, Sifan Liu, Yuanzhe Liu, Liang Wu, Xiaoyu Zhu, Xin Zou</b>
      </p>
      <p>
        In this project, we want to predict the creation year of a song. We explore and compare three machine learning models (Logistic Regression, Naive Bayes and Decision Tree) and one deep learning model (Multilayer Perceptron). The models are implemented on Microsoft Azure and AWS platform. Among these models, the Multilayer Perceptron gets the best performance with 61.8% accuracy.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1hyf9uHFaJ33bbWOrZ4vLjzskR9mrV1Jj/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->



      <!-- start group -->
      <h3 class="anchored">Music Genre Classification on Million Song Dataset</h3>
      <p>
        <b>Peter Shi, Shutan Wang, Mike Yao, Rick Zhou</b>
      </p>
      <p>
        The problem we are interested in is how different models performs on music genre classification according to different metrics. The ML methods we chose are Logistic Regression, Random Forest, Naive Bayes and Neural Network, with respect to metrics include accuracy, time, memory, precision by labels and ease of use. We use AWS EMR, EC2 as server, S3 as intermediate data storage, and Apache Spark, MLlib as our distributed ML environment. The final result shows that LR and NN have the highest accuracy, RF is more flexible for users to trade off between accuracy and time/memory, and Naive Bayes consumes smallest amount of time and memory but not perform well on accuracy since it is a simple probabilistic model. Some other interesting takeaways include some features, such as “artist name”, are more important in genre classifications(i.e have largest weight), and simple models may have great accuracy difference based on how features are pre-partitioned.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1zQ-PEtOx2jiRl7ci7-mocdMQULFX8X6h/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Song Popularity Prediction on Million Song Dataset</h3>
      <p>
        <b>Qian Chen, Sweta Priyadarshi, Xuefan Zha, Sammie Liang, Tyson Wang</b>
      </p>
      <p>
        In this project, our group creates a machine learning pipeline to predict the song popularity, also known as song hotness, for the million song dataset including 1,000,000 songs. After data cleaning, PCA analysis and feature engineering are performed to generate the training, validation and test dataset which are stored in the AWS S3 bucket. Linear regression, random forest and gradient boosted tree models are trained and evaluated through AWS EMR using Pyspark and MLlib. The model with the best performance is the gradient boosted tree model which has a mean absolute error less than 0.15. This pipeline demonstrates its effectiveness and accuracy for large scale dataset.
      </p>
      <center><iframe src="https://docs.google.com/presentation/d/e/2PACX-1vR7bjI4jGTJsd1rOjyMkPYiuDHVRAQNdC9AqxvFdnNT68g6Ybsn4FN6aGvxZfUm4cDEYhvroBMQj063/embed?start=false&loop=false&delayms=3000" frameborder="0" width="640" height="389" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Dimension-Reduction Methods for Image Classification</h3>
      <p>
        <b>Amy Lee, Vincent Mai, Yuchao Wu, Jessica Zhu</b>
      </p>
      <p>
        This project presents a systematic comparison of dimensionality reduction techniques in the context of large-scale image classification. Given the nature of large image datasets, it is not immediately clear what the best way to reduce the dimension of the images would be. Therefore, we chose to implement 3 different dimension-reduction techniques (<b>PCA</b>, <b>KPCA</b>, and <b>Deep Autoencoding</b>) and compare their performance with respect to selected metrics (e.g. runtime, memory usage, scalability, reconstruction quality, classification performance) using <b>Spark</b> and <b>Tensorflow</b>. Ultimately, we found that PCA had the best performance with respect to our metrics, but Deep Autoencoding was the most scalable. However, further study is needed to corroborate our results.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1Dq8FZSg_cXJ8l5jA6SD-1FKEobQki0wY/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->



      <!-- start group -->
      <h3 class="anchored">Temporal Topic models for Reddit Score Prediction</h3>
      <p>
        <b>Paola Buitrago, Alec Jasen, Eric Li, Karun Thankachan, Santiago Roa</b>
      </p>
      <p>
        We set out to see if, given the observable features at the time of the creation of a Reddit post, it be possible to predict the score it would eventually attain and to figure out the infrastructure and pipeline design required to support such a production-level large-scale ML system.
      </p>

      <p>
        To predict the score of Reddit we used the Reddit Post Dataset (300 GB), from which we extracted metadata information (time, subreddit) and content(title and post text) to create features (topics model at a month level, word2Vec embedding, sentiment) to train regression models - Linear Regression, Random Forests, and Gradient Boosted Trees. Their performance was compared using RMSE and R2 as metrics.
      </p>

      <p>
        The raw data was pulled in from Google BigQuery to the five node (n1-standard-4, 15GB Ram, 4vCPU) Dataproc cluster that we used to complete data processing (2 days and 6 hours for 200GB) and machine learning (~100s for linear regression, ~90s for the random forest, ~550s for GBTree)
      </p>

      <p>
        The results varied from subreddit to subreddit, ranging from RMSE of 0.09 (OneAmericaNews subreddit) to 8287 (gifs subreddit) and R2-values on appropriate subreddits of 0.16 to -0.12. (ethtrader to Futurology)
      </p>

      <p>
        We found that the heavily skewed scores (most scores are zero) and the temporal relevancy of Reddit posts require a more nuanced feature engineering approach. Also, we learned the phases of a production ML pipeline (from Data Ingestion to Model Scoring) and got a handle on estimating the infrastructure required to support it.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1rrUcziU2NXrDBqyAoR7YhhYkjC-YqCAn/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->



      <!-- start group -->
      <h3 class="anchored">Song Recommendation System on Million Song Dataset</h3>
      <p>
        <b>Deeptha Anil Kumar, Bhumi Dinesh Bhanushali, Varsha Kuppur Rajendra, Kathan Nilesh Mehta</b>
      </p>
      <p>
        Our main aim of the project was to build the ML pipeline for the Million Song Dataset and recommend top songs that the user would prefer to listen to in an optimized way. For this, we performed a comparative analysis of three Machine Learning techniques based on computation time and precision. These methods were - Popularity-based model, content-based model and collaborative model. We used AWS to load the data, extracted the CSV version of it and uploaded it to Microsoft Azure cloud platform for using it on scale with Databricks. Collaborative model gave the best precision results, followed by the content-based model and popularity-based model. In terms of computation time, the popularity-based model was naive, simple to implement and took the minimum time to train followed by the collaborative model and content-based model. Apart from the comparative study, we also learnt how to deal with real world large datasets and the majority of our time was dedicated to data loading, data cleaning and feature extraction rather than running each ML technique.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1n2YgacGxb8vaaSwrQkwcJ-ldP9-roNDC/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->



      <!-- start group -->
      <h3 class="anchored">NYC Taxi Fare Prediction</h3>
      <p>
        <b>Zeeshan Ahmed, Abhishek Bamotra, Vivek Gupta, Jimmy Herman, Baljit Singh, Pranav Thombre</b>
      </p>
      <p>
        We sought to answer the following questions: What is the projected total fare for a taxi trip given the time, date, and pickup & drop-off location? What features are most relevant in predicting a taxi fare? What degree of prediction accuracy is satisfactory? What other metrics should be considered when selecting our model? We evaluated 4 ML models: Linear Regression, Decision Tree, Random Forest and Gradient Boosted Trees. All of our work was completed on Spark using the ML library and Pyspark API. We used both Azure Databricks and AWS EMR for our project. We found the Random Forest model to have the lowest RMSE in fare prediction and the linear regression model had the fastest training time, lowest inference latency, smallest model size and was the most interpretable. However, across all metrics, the decision tree was the most balance model. We learned valuable lessons in handling large dataset, managing cloud resources/expenses, experienced distributed ML on AWS and Azure, configured Spark executors and mitigated failure scenarios and much more through this project.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/102d5-0u_kCtTJwDSdm5zNQOqP4lE9mRE/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Million Song Dataset for Music Mode and Genre Classification</h3>
      <p>
        <b>Yifan Chen, Qianwei Li, Tianyu Xu, Haisu Yu, Feifan Zhang</b>
      </p>
      <p>
        The major question we are trying to solve in this project is to use the million song dataset to evaluate the performance of different machine learning methods under specific contexts. Specifically, we are curious about whether it is possible to predict some general information about a song, such as mode and genre, when given some musical features and artist information about the song. Therefore, we have chosen four machine learning models, including logistic regression, random forest, gradient boost tree, and multi-layer perceptron to predict music mode and also classify different genres for songs. Multiple tools and platforms were used in this project, such as AWS S3 and Azure Databricks.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/19IKJGorzltQDrkIPJDkbb5UzHeg68M3F/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


      <!-- start group -->
      <h3 class="anchored">Finding Patterns in Darknet Market Listings</h3>
      <p>
        <b>Arthur Dzieniszewski, Gregory Howe, Jennifer Lee, Ulani Qi, Alexander Schneidman</b>
      </p>
      <p>
        Our goal for this project was to find interpretable patterns or useful groupings among the darknet market listings. To solve this problem, we used several dimensionality reduction techniques like PCA, tSNE, and VDSH. We ran all of our computation and data preprocessing on AWS EMR clusters and AWS CUDA instances. Our most successful technique, VDSH, revealed that the listings could be clustered into smaller groups where the nearest neighbor would be a listing for a similar product.
      </p>
      <center><iframe iframe frameborder="0" scrolling="no" src="https://drive.google.com/file/d/1DUdNLq2Gg7Ze-shEIoxciIVtNj_zaY_o/preview" width="640" height="380"></iframe></center>
      <p>&nbsp;</p>
      <!-- end group -->


    </div>
    <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>
    <script>
      anchors.options.visible = 'always';
      anchors.add('.anchored');
    </script>


</body>
