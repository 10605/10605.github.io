# Database of info on slide decks for individual "modules"
# slides themselves stored on William's Dropbox

minhash-and-index.pptx:
  num_slides: 13
  summary: Jaccard, MinHash compared to SimHash, multihash indexing

spork.pptx:
  num_slides: 3
  summary: simplified but constant-memory version of Spark
mr-origins-hazsoup.py:
  num_slides: 27
  summary: unix hacks, sort, map-reduce va hazsoup

ppi.pptx:
  num_slides: 50
  summary: discussion, info cascade, watts science paper, schelling, bayesian ppi, stratppi
  links:
   - https://www.cs.cornell.edu/home/kleinber/networks-book/: Information cascades, Ch 16, Easly and Kleinberg, 2010
   - https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1756-8765.2009.01030.x: Salganic, Dodds, Watts, Science 2006
   - http://nifty.stanford.edu/2014/mccown-schelling-model-segregation/: Schelling Spatial Segregation model
   - https://www.science.org/doi/full/10.1126/science.adi6000?casa_token=UKBrYB_jH6gAAAAA%3AcBNEc67rQIRWJqH_d3O_jFGMdyF3EZlz6R_dhEUQefUIilA5CpzRRHmv5OqBlXQ7YCFTfmM9x8E8bQ: Prediction Powered Inference, Angelopolous 2023, Science
   - https://arxiv.org/abs/2405.06034: Bayesian PPI, Hofer et al 2024
   - https://arxiv.org/pdf/2305.06984: Kamaloo et al, 2023
   - https://proceedings.neurips.cc/paper_files/paper/2024/file/c9fcd02e6445c7dfbad6986abee53d0d-Paper-Conference.pdf: Fisch et al, 2024

branch-train-merge.pptx:
  num_slides: 22
  summary: BTM paper, background on MoE, BTX paper
  links:
    - https://arxiv.org/pdf/2208.03306: Branch-train-merge paper
    - https://arxiv.org/pdf/2403.07816: Branch-train-mix paper
task-vectors.pptx:
  num_slides: 35
  summary: word2vec analogies, task vector paper, disentanglement definition, TIES, 
  links:
    - https://aclanthology.org/W14-1618.pdf: Levy and Goldberg demystification of word2vec (1/2) 2014
    - https://arxiv.org/pdf/1402.3722: Levy and Goldberg demystification of word2vec (2/2) 2014
    - https://arxiv.org/pdf/2212.04089: Task arithmetic paper, 2023
    - https://proceedings.neurips.cc/paper_files/paper/2023/file/d28077e5ff52034cd35b4aa15320caea-Paper-Conference.pdf: Weight disentanglement and neural tangent kernel paper, 2023
    - https://www.researchgate.net/profile/Leshem-Choshen-2/publication/371291070_Resolving_Interference_When_Merging_Models/links/6481cc4a79a722376518d1d6/Resolving-Interference-When-Merging-Models.pdf: TIES-merging paper, 2023
    - https://arxiv.org/pdf/2307.13269: LoRA hub paper, 2024
    - https://arxiv.org/pdf/2402.05859: PHATGOOSE paper, 2024
rag-fid-recap.pptx:
  num_slides: 25
  summary: recap DPR, Contreiver, RAG, FiD, FiDO, LUMEN, and present GLIMMER
fid-genllm.pptx:
  num_slides: 10
  summary: PCW, up thru dynamic block-sparse attention
  links:
   - https://arxiv.org/pdf/2212.10947: Parallel Context Windows, 2023
   - https://arxiv.org/pdf/2409.15355: Block-Attention for Efficient RAG, 2024
   - https://arxiv.org/pdf/2503.08640: Dynamic Block-Sparse Attention paper, 2025
rag-recap.pptx:
  num_slides: 15
  summary: recap of dpr, contreiver
rag-genllm.pptx:
  num_slides: 9
  summary: echo embeddings and promptEOL
  links:
   - https://aclanthology.org/2023.acl-long.99.pdf: HyDE paper
   - https://arxiv.org/pdf/2304.14233: LameR paperx
   - https://openreview.net/pdf?id=Ahlrf2HGJR: Echo embedding paper
   - https://arxiv.org/pdf/2307.16645: PromptEOL paper
fido-etc.pptx:
  num_slides: 32
  summary: fido, lumen, glimmer, operational intensity
  links:
    - https://arxiv.org/pdf/2212.08153: FiDO paper
    - https://arxiv.org/abs/1911.02150: Shazeer paper analyzing operational intensity
    - https://proceedings.mlr.press/v202/de-jong23a/de-jong23a.pdf: LUMEN paper
    - https://arxiv.org/pdf/2306.10231: GLIMMER paper
rag-fid.pptx:
  num_slides: 16
  summary: og rag paper, FiD paper
  links:
    - https://arxiv.org/pdf/2005.11401: RAG for knowledge-intensive NLP tasks, 2020
    - https://proceedings.mlr.press/v119/guu20a.html?ref=http://githubhelp.com: REALM paper, 2020
    - https://arxiv.org/abs/2007.01282: Fusion in decoder paper, 2021
    - https://arxiv.org/pdf/2212.08153: FiDO paper, 2023
    - https://arxiv.org/pdf/1911.02150: Shazeer paper on optimizing decoder inference with multihead queries
    - https://arxiv.org/pdf/2212.08153: LUMEN paper, 2023
    - https://arxiv.org/pdf/2212.08153: GLIMMER paper, 2023
    - https://arxiv.org/pdf/2212.10947: Parallel context windows, 2023
    - https://arxiv.org/pdf/2409.15355: Block-attention for RAG, 2024
    - https://arxiv.org/pdf/2503.08640: Dynamic block-sparse attention for ICL, 2025
rag-conflicts.pptx:
  num_slides: 11
  summary: Sameer paper on knowledge conflicts
rag-overview.pptx:
  num_slides: 8
  summary: RAG pipeline at high-level
  links:
    - https://dl.acm.org/doi/abs/10.1145/3078971.3078983: DeepHash paper, 2017
contrastive-overview.pptx:
  num_slides: 18
  summary: overview of contrastive learning, DPR, contriever, a few comments about ColBERT and EntityQuestions
contrastive-dpr.pptx:
  num_slides: 7
  summary: DPR
  links:
    - https://arxiv.org/pdf/2004.04906v2/1000: DPR paper, 2020
contrastive-tricks.pptx:
  num_slides: 20
  summary: mega-batching, hard negatives, in-batch negatives, queues, moco
  links:
   - https://arxiv.org/pdf/2104.08253: Condenser paper
   - https://arxiv.org/pdf/2109.08535: EntityQuestions paper
   - https://dl.acm.org/doi/abs/10.1145/3397271.3401075: ColBERT paper
   - https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf: MoCo paper
contriever.pptx:
  num_slides: 11
  summary: contriever
  links:
   - https://arxiv.org/pdf/2112.09118: Contreiver paper
   - https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf: MoCo paper
model-compression-recap.pptx:
  num_slides: 35
  summary: looking back at pruning, quantization, and kv-caching as "learning"
structured-pruning.pptx:
  num_slides: 20
  summary: summary of a number of pruning papers
  links:
    - https://proceedings.neurips.cc/paper_files/paper/2023/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf: LLM Pruner, 2023
    - https://openreview.net/forum?id=18VGxuOdpu: Shortened Llama, ... Kim et al 2024
    - https://arxiv.org/pdf/2310.06694: Sheared LLama ... , Xia et al, 2024
    - https://arxiv.org/pdf/1510.00149: Han et al 2016

kv-cache-transformer-2-short.pptx:
  num_slides: 28
  summary: review of H2O, StreamingLLM, SnapKV, FastGen
  links:
    - https://proceedings.neurips.cc/paper_files/paper/2024/hash/28ab418242603e0f7323e54185d19bde-Abstract-Conference.html:
        "SnapKV: LLM Knows What You are Looking for Before Generation, Li et al 2024"
    - https://arxiv.org/abs/2310.01801: "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs, Ge et al 2023"

kv-cache-transformer-2.pptx:
  num_slides: 47
  summary: H2O and StreamLLM #to add SnapKV
  links:
    - https://arxiv.org/abs/2004.08483: "ETC: Encoding Long and Structured Inputs in Transformers, Ainsle et al 2020"
    - https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html:
        "Big Bird: Transformers for Longer Sequences, Zaheer et al, 2020"
    - https://arxiv.org/abs/2001.04451: "Reformer: the Efficient Transformer, Kitaev et al, 2020"
    - https://proceedings.neurips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html:
        "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models, Zhang et al 2023"
    - https://arxiv.org/pdf/2309.17453: Efficient streaming LMs with attention sinks, Xiao et al 2024
    - https://proceedings.neurips.cc/paper_files/paper/2024/hash/28ab418242603e0f7323e54185d19bde-Abstract-Conference.html:
        "SnapKV: LLM Knows What You are Looking for Before Generation, Li et al 2024"

kv-cache-transformer-1.pptx:
  num_slides: 25 # some long animations
  summary: motivating and explaining kv-caches
cats-and-sentiment.pptx:
  num_slides: 19
  summary: cat neuron and face detector neuron from Ng, sentiment neuron from OpenAI
  links:
    - https://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf: The cat neuron paper
    - https://arxiv.org/pdf/1704.01444: Learning to Generate Reviews and Discovering Sentiment, Radford et al 2017
pruning.pptx:
  num_slides: 42
  summary: summary of a number of pruning papers
  links:
    - https://arxiv.org/pdf/1710.01878: To prune or not to prune ..., Zhu and Gupta 2017
    - https://arxiv.org/abs/2306.11695: A simple and effective approach ... (the Wanda paper), Sun et al
    - https://arxiv.org/abs/2104.08378: Accelerating Sparse Deep Neural Networks, Mishra et al 2021
    - https://proceedings.neurips.cc/paper_files/paper/2023/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf: LLM Pruner, 2023
    - https://openreview.net/forum?id=18VGxuOdpu: Shortened Llama, ... Kim et al 2024
    - https://arxiv.org/pdf/2310.06694: Sheared LLama ... , Xia et al, 2024
    - https://arxiv.org/pdf/1510.00149: Han et al 2016
quantize.pptx:
  num_slides: 17
  summary: quantization and distillation
  links:
   - https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html: Raschka blog post, 2023
   - https://leimao.github.io/article/Neural-Networks-Quantization/: Lei Mao blog post, 2023
   - https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf: LLM.int8(), Dettmers et al 2022
   - https://openreview.net/forum?id=i8tGb1ab1j: Dettmers and Zettlemoyer 2023
   - https://proceedings.neurips.cc/paper_files/paper/2024/file/091166620a04a289c555f411d8899049-Paper-Conference.pdf: PV-Tuning, Malinovskii et al, 2024
quantize-distill.pptx:
  num_slides: 17
  summary: quantization and distillation
  links:
   - https://arxiv.org/pdf/1503.02531: Distilling the Knowledge in a Neural Network, Hinton, Vinyals, Dean, 2015
   - https://dl.acm.org/doi/pdf/10.1145/1150402.1150464: Model Compression, Bucila et al, 2006
   - https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html: Raschka blog post, 2023
   - https://leimao.github.io/article/Neural-Networks-Quantization/: Lei Mao blog post, 2023
   - https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf: LLM.int8(), Dettmers et al 2022
   - https://openreview.net/forum?id=i8tGb1ab1j: Dettmers and Zettlemoyer 2023
   - https://proceedings.neurips.cc/paper_files/paper/2024/file/091166620a04a289c555f411d8899049-Paper-Conference.pdf: PV-Tuning, Malinovskii et al, 2024
transformer-bpe-short.pptx:
  num_slides: 11
  summary: vision transformers and murag
vizier.pdf:
  num_slides: 49
  summary: Vizier talk from Greg
  links:
  - https://drive.google.com/file/d/1T4s3CPBHazGhGFZpBcLa6HtnvS7PRcMT/view?usp=drive_link: PDF of Greg's slide deck  
hyperparam.pptx:
  num_slides: 35
  summary: overview of grid, adaptive, random search, hyperband
  links:
   - https://arxiv.org/abs/2203.15556:
       Hoffman et al (2022). Training Compute-Optimal LLMs
   - https://openreview.net/forum?id=xI71dsS3o4: (Mis)fitting - a study of scaling laws, ICLR 2025
   - https://www.jmlr.org/papers/v18/16-558.html: Hyperband - A Novel Bandit Based Approach to Hyperparameter Optimization, JMLR 2018
lora-recap.pptx:
  num_slides: 9
  summary: recap of lora, how it's use for transformers/BERT
transformer-kv-intro.pptx:
  num_slides: 10
  summary: why key-value caching works
  links:
    - https://medium.com/@joaolages/kv-caching-explained-276520203249: João Lages blog post
transformer-bpe.pptx:
  num_slides: 21
  summary: BPE, wordpieces, ViT tokenization
  links:
  - https://arxiv.org/pdf/1508.07909: Neural Machine Translation of Rare Words with Subword Units, Sennrich et al 2016
  - https://huggingface.co/docs/transformers/tokenizer_summary: Huggingface's guide to BPE and similar
  - https://arxiv.org/pdf/2010.11929/1000: 
     'An Image is Worth 16x16 Words: Transformers for Image Recognition, ICLR 2021'
  - https://arxiv.org/abs/2210.02928: MuRag Multimodal Retrieval-Augmented Generation ..., Chen et al 2022
transformer.pptx:
  num_slides: 33
  summary: walkthru of transformers 
  links:
    - https://nlp.seas.harvard.edu/annotated-transformer/: The Annotated Transformer
    - https://github.com/rasbt/LLMs-from-scratch: LLMs from scratch book on Github
    - https://e2eml.school/transformers.html: A bit like the annotated transformer but with more depth in some areas
    - https://dl.acm.org/doi/abs/10.5555/2627435.2670313: Dropout paper, Srinivasta et al JMLR 2014
    - https://arxiv.org/abs/1607.06450: LayerNorm paper,  Ba et al, 2016.
evolution-of-transformers-summary.pptx:
  num_slides: 11
  summary: quick overview of key points
transformer-discussion.pptx:
  num_slides: 8
  summary: some high-level observations
evolution-of-transformers.pptx:
  num_slides: 97
  summary: history of DL
  links:
    - http://neuralnetworksanddeeplearning.com/: online book on deep networks
    - https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf: 
       Understanding the difficulty of training deep feedforward neural networks, Glorot and Bengio, AIStats 2010
    - https://deeplizard.com/resource/pavq7noze2: Convolution demo
    - https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html:
       Deep Residual Learning for Image Recognition, He et al, CVPR 2016
    - https://arxiv.org/abs/1301.3781: 
       Efficient Estimation of Word Representations in Vector Space, Mikolov et al, 2013
    - https://projector.tensorflow.org/: Visualize Word2Vec
    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/: Karpathy blog post on LSTMs
    - https://wtf-deeplearning.github.io/machine-translation/1409.0473.pdf:
        NMT by jointly learning to align and translate, 2016

dl_finetuning.pptx:
  num_slides: 20
  summary: Ginger's slides on PEFT, including additions about BERT
  links:
    - https://drive.google.com/file/d/1-qTFQw7rW1rIsGf8kN4-pxv_edbfFfmp/view?usp=sharing:
       Previous class lecture on Oct 21 2024
distributed_dl.pptx:
  num_slides: 57
  summary: Ginger's slides on GPU-related optimization, includes ring all-reduce
  links:
    - https://drive.google.com/file/d/1-MLmngnoJAAgjVm4CpCLXcjm6WQsjFkU/view?usp=sharing: 
       Previous class lecture on Nov 4 2024
gpu-programming-short.pptx:
  num_slides: 40 #newer, not really shorter
  summary: cuda !gpu | GPUS, intro to minibatch SGD, Cuda/Pytorch examples
  links:
    - https://www.youtube.com/watch?v=-P28LKWTzrI: Mythbusters explain GPUs
    - https://ourworldindata.org/grapher/gpu-price-performance: GFlops / dollar up to 2022
    - https://github.com/10605/LectureSampleCode/blob/main/autodiff/gpu.py: code for benchmarking GPUs
    - https://drive.google.com/file/d/1-qTFQw7rW1rIsGf8kN4-pxv_edbfFfmp/view?usp=sharing: Fall 2024 10-605 lecture on ML hardware
bloom-filter-countmin-recap.pptx:
  num_slides: 9
  summary: review of math for both
lsh-knn-pq.pptx:
  num_slides: 12
  summary: LSH, multi-hash indices, k-NN indices, PQ indices
  links:
    - https://arxiv.org/pdf/1307.2982:
        Fast Exact Search in Hamming Space with Multi-Index Hashing, 2014
    - https://arxiv.org/abs/1908.10396:
        Accelerating Large-Scale Inference with Anisotropic Vector Quantization, 2020
    - https://arxiv.org/pdf/2401.08281:
        The FAISS library, 2024
short-faithful-kb-queries.pptx:
  num_slides: 37
  summary: EmQL paper with Haitian
  links:
    -  https://proceedings.neurips.cc/paper/2020/hash/fe74074593f21197b7b7be3c08678616-Abstract.html:
         Faithful KB Embeddings - Sun et al, 2020
param-server-short.pptx:
  num_slides: 6
  summary: param server main idea
short-matrix-factorization-via-distributed-sgd.pptx:
  num_slides: 14
  summary: second part of Gemulla's paper
  links:
    - https://dl.acm.org/doi/abs/10.1145/2020408.2020426:
        Large-scale matrix factorization with distributed stochastic gradient descent - Gemulla et al 2011
k-means-recap.pptx:
  num_slides: 6
  summary: quick recap of k-means algorithm
autodiff-writeup.pptx:
  num_slides: 29
  summary: detailed walkthru of my code
sparse-regularized-sgd-reveal.pptx:
  num_slides: 6
  summary: answers questions raised in sparsity-linreg-and-sgd.pptx
hash-kernels-linreg.pptx:
  num_slides: 17
  summary: hash trick, two langford papers
  links:
    - https://proceedings.mlr.press/v5/shi09a.html:
        Hash Kernels, PMLR 2009
    - https://dl.acm.org/doi/abs/10.1145/1553374.1553516:
        Feature hashing for large scale multitask learning, ICML 2009
parallel-ml-overview.pptx:
  num_slides: 16
  summary: data/model parallism, some extremes, iterative parameter mixing
  links:
    - https://proceedings.neurips.cc/paper_files/paper/2011/hash/218a0aefd1d1a4be65601cc6ddc1520e-Abstract.html:
        Hogwild, A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, Rech et al, 2011
    - https://fasttext.cc/:
        FastText classification library
    - https://arxiv.org/abs/1607.01759:
        Bag of Tricks for Efficient Text Classification, Joulin et al 2016
    - https://arxiv.org/abs/2003.06307:
        Communication-Efficient Distributed Deep Learning, Tang et al 2023
    - https://aclanthology.org/N10-1069.pdf:
        Distributed Training Strategies for the Structured Perceptron, McDonald et al, 2010
sparsity-linreg-and-sgd.pptx:
  num_slides: 11
  summary: sparsity updates for linreg and logistic regression, sparsity and overfitting, no sparse-update tricks
  links: 
  - https://www.cs.cmu.edu/~wcohen/10-605/notes/sgd-notes.pdf:
      "William's notes on SGD for Logistic Regression and Sparsity"
linreg-kernels.pptx:
  num_slides: 16
  summary: kernels linreg, poly and gaussian kernels, exact kernel ridge regression
  links:
    - https://drive.google.com/file/d/175P2ftDaS9-diVjO7I8oC57PoGQJKg8B/view:
        Previous class lecture on Sept 16 2024
ridge-reg-and-basis-vecs.pptx:
  num_slides: 13
  summary: ridge regression, basis vectors, background needed for HW2 S25 601
linreg-to-logreg-sgd.pptx:
  num_slides: 7
  summary: derive sgd update for logreg, point out similarity to linreg
sgd-and-parallel-linear-regression.pptx:
  num_slides: 11
  summary: sgd background, derivation for linear regression
batch-gradient-linear-regression.pptx:
  num_slides: 14
  summary: linreg with batch, plus parallel gradient descent
  links:
    - https://drive.google.com/file/d/16mnJaGcfhiEXLEcvWywo01Ew4Dpc-coM/view:
        Previous class lecture  on Sept 9 2024
    - https://drive.google.com/file/d/16mnJaGcfhiEXLEcvWywo01Ew4Dpc-coM/view:
        Previous class lecture on Sept 11 2024
learning-linreg-as-optimization.pptx:
  num_slides: 19
  summary: warmup with bernoulli, exact linreg derivation
matmul-in-spark.pptx:
  num_slides: 4
  summary: matmul in spark
k-means-in-spark-example.pptx:
  num_slides: 12
  summary: quick summary of k-means in spark 
iterative-pagerank-in-spark.pptx:
  num_slides: 7
  summary: slow and fast implementations
comparing-corpora.pptx:
  num_slides: 11
  summary: phraseness pipeline
  links:
  - https://aclanthology.org/W03-1805.pdf:
      A Language Model Approach to Keyphrase Extraction
601-bayes.pptx:
  num_slides: 23
  summary: bayes rule, smoothing as MAP
601-prob-intro.pptx:
  num_slides: 52
  summary: motivation, axioms, independence, conditional prob, smoothing
605-administrivia.pptx:
  num_slides: 12
  summary: 'overview of 10-605: grading policies, projects, 605 vs 805 | Grading policies
    and etc'
605-overview+prob-review-recap.pptx:
  num_slides: 2
  summary: ''
605-overview-recap.pptx:
  num_slides: 6
  summary: ''
adagrad.pptx:
  num_slides: 7
  summary: ''
alias-fenwick-tree-sampling-lda.pptx:
  num_slides: 10
  summary: '!lda mimno''s sparse LDA'
tree-allreduce.pptx:
  num_slides:
  summary: allreduce walkthru
  links:
  - https://www.jmlr.org/papers/volume15/agarwal14a/agarwal14a.pdf:
      Agarwal et al, A Reliable Effective Terascale Linear Learning System
allreduce.pptx:
  num_slides: 14
  summary: All-reduce
autodiff-details.pptx:
  num_slides: 4
  summary: '!deep !autodiff | Inputs, parameters, updates'
autodiff-systems.pptx:
  num_slides: 5
  summary: Theano and some other examples !autodiff | Some systems using autodiff
autodiff.pptx:
  num_slides: 35
  summary: ' +autodiff | Reverse-mode differentiation (autodiff)'
  links:
    - https://justindomke.wordpress.com/2009/03/24/a-simple-explanation-of-reverse-mode-automatic-differentiation/:
       A simple explanation of reverse-mode automatic differentiation, Justin Domke
    - https://github.com/10605/LectureSampleCode/tree/main/autodiff:
       code for autodiff with Wengert lists
autodiff.txt:
  num_slides: 0
  summary: writeup of autodiff assignment | Symbolic reverse-mode differentiation
averaged-perceptron.pptx:
  num_slides: 4
  summary: ' | Averaged perceptrons'
bigdata-history.pptx:
  num_slides: 16
  summary: +rep, ripper, banko & brill, affect/effect demo, deep learning requires
    big data | History of Big Data
  links:
  - https://wwcohen.github.io/postscript/ijcai-93.ps:
      William W. Cohen (1993). Efficient pruning methods for separate-and-conquer rule learning systems
  - https://aclanthology.org/P01-1005.pdf:
      Banko, Michele, and Eric Brill (2001). Scaling to very very large corpora for natural language disambiguation
  - https://books.google.com/ngrams/:
      Google NGrams Viewer
  - https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804817&casa_token=ILyj04MU_08AAAAA:QZtVl6LIhMMKqXSd8moyUfbM2hv4F2Zje-FbQMMvm1vnW0pQz0MK5A4BkQpbXdSr2x8jsULS&tag=1:
      Norvig, Pereira, Halevy (2009). The Unreasonable Effectiveness of Data
  - https://www.nowpublishers.com/article/Details/MAL-006:
      Yoshua Bengio (2009). Learning Deep Architectures for AI
  - https://github.com/10605/LectureSampleCode/tree/main/ngrams: 
      code for doing ngram queries
bloom-filter-countmin-review.pptx:
  num_slides: 14
  summary: +bloom | Review of Bloom filters
bloom-filter-recap.pptx:
  num_slides: 6
  summary: '!bloom !countmin recap of bloom-filter.pptx | Review of Bloom filters'
bloom-filter.pptx:
  num_slides: 15
  summary: +bloom | Bloom filters
  links:
    - https://www.cs.cmu.edu/~wcohen/10-605/notes/randomized-algs.pdf:
        "William's Notes on Randomized Algorithms"
    - https://github.com/10605/LectureSampleCode/blob/main/randomized/bloomfilter.py:
        code for Bloom Filter 
    - https://openreview.net/pdf?id=S1hsDCNFx:
        'Short and Deep: Sketching and Neural Networks, Daniely et al'
chinchilla-scaling.pptx:
  num_slides: 7
  summary: Data and Compute Scaling for LLMs
  links:
  - https://arxiv.org/abs/2203.15556:
      Hoffman et al (2022). Training Compute-Optimal LLMs
cnn.pptx:
  num_slides: 20
  summary: '!deep | Convolutional ANNs'
common-hadoop-extensions-detailed.pptx:
  num_slides: 30
  summary: hadoop streaming, mrjob, cascading, hive, pig, iteration in pig, spark,
    flink +pig +spark | Systems built on top of Hadoop
common-hadoop-extensions-short.pptx:
  num_slides: 12
  summary: cascading, hive vs pig, flink !pig | Systems built on top of Hadoop
comparing-parallel-nb-and-perceptrons.pptx:
  num_slides: 6
  summary: also discusses structured perceptrons, rocchio
complexity.pptx:
  num_slides: 9
  summary: Complexity theory review
cost-of-operations.pptx:
  num_slides: 7
  summary: Cost of Disk/Memory Access
  links:
  - https://samwho.dev/numbers/:
      Visualizing the cost of operations
  - https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage?time=1990..latest:
      Historical cost of storage
  - https://github.com/erikfrey/bashreduce:
      code for Bash Reduce - a minimal implementation of Map-Reduce
  - https://github.com/10605/LectureSampleCode/tree/main/hazsoup:
      code for Hazsoup - a more readable minimal implementation of Map-Reduce
construct-wengert-list.pptx:
  num_slides: 6
  summary: constructing a wengert list
convnets.pptx:
  num_slides: 23
  summary: ' | Convolutional ANNs'
count-min-recap.pptx:
  num_slides: 4
  summary: recap of count-min | Review of count min
count-min-sketch.pptx:
  num_slides: 11
  summary: +countmin | The countmin sketch
  links:
    - https://drive.google.com/file/d/17OyCk5jMM1Kfjt3MuQDm00DJWJoBCuxi/view:
        Previous course lecture, 9/25/2024
deep-learners-have-unstable-gradients.pptx:
  num_slides: 10
  summary: '!deep | Exploding and vanishing gradients'
deep-learning-intro-and-history.pptx:
  num_slides: 10
  summary: +deep | Deep learning intro
deep-networks-and-sgd.pptx:
  num_slides: 2
  summary: '!deep | trainability with SGD'
deep-networks-are-expressive.pptx:
  num_slides: 4
  summary: '!deep | Expressiveness of MLPs'
deep-recap.pptx:
  num_slides: 4
  summary: recap of backprop algorithm, mostly
deep-sketches.pptx:
  num_slides: 11
  summary: ' | CM Sketches in Deep Learning'
deephash:
  num_slides: 17
  summary: '!lsh | DeepHash'
  links:
    - https://dl.acm.org/doi/abs/10.1145/3078971.3078983: DeepHash paper, 2017
delta-trick-for-vp.pptx:
  num_slides: 3
  summary: ' | The "delta trick"'
density-vs-naive-bayes.pptx:
  num_slides: 12
  summary: Brute force for ngrams data, multinomial NB
density-estimation-by-counting.pptx:
  num_slides: 15
  summary: '  joint, affect/effect demo, counting experiment, density estimation vs
    classifier | Counting for big data and density estimation'
dgm-review.pptx:
  num_slides: 6
  summary: +dgm
eval-and-diff-wengert-list.pptx:
  num_slides: 11
  summary: '   !autodiff | Details on Wengert lists'
expander.pptx:
  num_slides: 4
  summary: ' | Google''s Expander SSL System'
generative-models-for-text.pptx:
  num_slides: 20
  summary: '!nbayes define DGMs, model for naive bayes and LDA | DGMs for naive Bayes
    and LDA'
gibbs-for-lda.ppt:
  num_slides: 15
  summary: '!lda +gibbs-lda +gibbs | Gibbs sampling for LDA'
gpu-intro.pptx:
  num_slides: 8
  summary: gpu +gpu | Introduction to GPUs
gpu-programming-intro.pptx:
  num_slides: 38
  summary: cuda !gpu | CUDA
gpus-vs-hadoop.pptx:
  num_slides: 5
  summary: gpus | GPUs vs Hadoop
graph-ml-outtro.pptx:
  num_slides: 3
  summary: '!graph-arch'
graph-ml-overview-gibbs.pptx:
  num_slides: 6
  summary: '!gibbs +graph-arch | Graph-based ML architectures'
graph-ml-overview-nogibbs.pptx:
  num_slides: 5
  summary: +graph-arch | Graph-based ML architectures
graph-sampling-paper.pptx:
  num_slides: 6
  summary: jure/christos's empirical study of graph sampling methods | Sampling a
    graph
graphchi.pptx:
  num_slides: 12
  summary: '!graph-arch !graphlab | GraphChi'
graphlab.pptx:
  num_slides: 6
  summary: '!graph-arch +graphlab | GraphLab'
graphx.pptx:
  num_slides: 9
  summary: '!graph-arch !graphlab !spark | GraphX'
guinea-pig.pptx:
  num_slides: 10
  summary: wc example, join, group +gpig | Guinea Pig intro
hadoop-as-parallel-stream-and-sort.pptx:
  num_slides: 7
  summary: '  like imaginary-hadoop-assignment'
hadoop-combiners.pptx:
  num_slides: 6
  summary: +combiners | Combiners
hadoop-debugging.pptx:
  num_slides: 8
  summary: ' | Debugging Hadoop'
hadoop-intro.ppt:
  num_slides: 52
  summary: 'note: long section - basically alona''s hadoop intro: MR visualization,
    robustness; HDFS; hadoop streaming; wordcount example; combiners todo: update
    screen shots?'
hadoop-short-intro.pptx:
  num_slides: 20
  summary: basics and hdfs, lots of screen shots | Intro to Hadoop
hadoop-streaming-api.pptx:
  num_slides: 17
  summary: also quick overview of non-streaming | Hadoop Streaming
harmonic-fields.pptx:
  num_slides: 11
  summary: harmonic fields !ssl | HF and NELL as an example
hash-kernels.pptx:
  num_slides: 7
  summary: ' | Hash kernels'
hash-trick-review.pptx:
  num_slides: 6
  summary: ''
hash-trick-to-bloom-filter.pptx:
  num_slides: 6
  summary: ''
hashed-logreg.pptx:
  num_slides: 14
  summary: motivation, main idea, implementation suggestions | Hash kernels for logistic
    regression
hf-discussion.pptx:
  num_slides: 10
  summary: all the different derivations of it including co-training..... | Harmonic
    fields
imaginary-hadoop-assignment.pptx:
  num_slides: 17
  summary: '| How would you parallelize stream and sort?'
intro-to-graphs.pptx:
  num_slides: 10
  summary: follows underlying theme "how do you sample"?
iterative-pagerank-in-pig-and-gpig.pptx:
  num_slides: 8
  summary: '!pig !gpig | PageRank in Pig and Guinea Pig'
iterative-pagerank-in-pig.pptx:
  num_slides: 3
  summary: '!pig | PageRank in Pig'
k-means-in-pig-example.pptx:
  num_slides: 17
  summary: ends with discussion of i/o costs <spark-overview.pptx !pig | K-means in
    Pig
kernel-perceptron.pptx:
  num_slides: 7
  summary: ''
lda-in-python-recap.pptx:
  num_slides: 5
  summary: '!lda review of implementation of LDA'
lda-in-python.ppt:
  num_slides: 11
  summary: '!gibbs-lda !lda'
lda-like-graph-models.pptx:
  num_slides: 9
  summary: '!lda | DGMs for graphs'
lda-param-server.pptx:
  num_slides: 11
  summary: '!ps !lda | LDA Sampler with PS'
lda-review.pptx:
  num_slides: 4
  summary: '!lda review of LDA'
lda.ppt:
  num_slides: 10
  summary: '!nb-dgm +lda'
learning-as-optimization-short.pptx:
  num_slides: 4
  summary: differentiate to find max for binomial | Learning as optimization
local-partitioning-paper+assignment.pptx:
  num_slides: 33
  summary: pagerank-nibble paper + breakdown of the assignment; follows graph-sampling
    paper | Local partitioning
lsh.pptx:
  num_slides: 13
  summary: includes one slide of "applications" which are sort of vague really | Locality
    sensitive hashing
mad-discussion.pptx:
  num_slides: 16
  summary: MAD and SSL as optimization +mad | Modified Adsorption SSL method
mad-with-countmin-sketch.pptx:
  num_slides: 7
  summary: '!mad !countmin | MAD with countmin sketches'
managed-comm-in-param-server.pptx:
  num_slides: 29
  summary: -ps +lasso +coord-descent | Managed Communication in PS
manifold-trick-for-pic.pptx:
  num_slides: 15
  summary: '!spectral~average !pic !spectral~average | Label propagation for clustering
    non-graph data'
manifold-trick-for-ssl.pptx:
  num_slides: 10
  summary: '!spectral~average !ssl | Label propagation for SSL on non-graph data'
map-reduce-abstractions.pptx:
  num_slides: 9
  summary: examples from naive bayes testing | Abstractions for map-reduce 
map-reduce-frameworks.pptx:
  num_slides: 10
  summary: Flume and others, ptr to BashReduce
map-vs-reduce-side-joins.pptx:
  num_slides: 7
  summary: +mapside-join | Joins in Hadoop
matrix-backprop.pptx:
  num_slides: 14
  summary: ' bprop with square loss and logistic units | BackProp following Nielson'
matrix-factorization-defined.pptx:
  num_slides: 19
  summary: abstract definition, collab filtering, image modeling, modeling text |
    Matrix factorization
matrix-factorization-via-distributed-sgd-short.pptx:
  num_slides: 11
  summary: from gemulla's slide deck, follows matrix-factorization-via-sgd.pptx |
    distributed matrix factorization with SGD
matrix-factorization-via-distributed-sgd.pptx:
  num_slides: 30
  summary: from gemulla's slide deck, follows matrix-factorization-via-sgd.pptx |
    distributed matrix factorization with SGD
matrix-factorization-via-sgd.pptx:
  num_slides: 15
  summary: from gemulla's slide deck | Matrix factorization with SGD
matrix-factorization-vs-clustering.pptx:
  num_slides: 2
  summary: compare k-means and MF
mcdonald-structured-parallel-perceptron-paper.pptx:
  num_slides: 20
  summary: including analysis +ipm | Iterative parameter mixing paper
midterm-review.pptx:
  num_slides: 14
  summary: ' | Midterm review'
mitchell-backprop.pptx:
  num_slides: 14
  summary: bprop with square loss and logistic units +mitchell-bprop | BackProp following
    Mitchell
modern-deep-network-tricks.pptx:
  num_slides: 18
  summary: reLU, cross entropy loss (no derivation), initialization | Modern deep
    learning models
motivating-streaming.pptx:
  num_slides: 7
  summary: '| Why streaming is important'
mrw-paper.pptx:
  num_slides: 13
  summary: frank's results | Multirank-walk SSL method
naive-bayes-as-dgm.pptx:
  num_slides: 12
  summary: '!dgm !nbayes +nb-dgm also presents unsupervised naive bayes | DGMs for
    naive Bayes'
naive-bayes.pptx:
  num_slides: 26
  summary: stupid version w/ all attributes independent, streaming naive bayes algorithm
    +nbayes | streaming Naive Bayes
online-lsh.pptx:
  num_slides: 19
  summary: 'online lsh for word embedding like things, pooling | Online LSH'
other-work-with-phrases.pptx:
  num_slides: 10
  summary: turney semantic orientation, downey long NE finding !phrases | Other work
    with phrases
pagerank-intro.pptx:
  num_slides: 4
  summary: +pagerank | PageRank
pagerank-via-stream-and-sort.pptx:
  num_slides: 9
  summary: follows pagerank-intro
pagerank-with-prs-on-disk.pptx:
  num_slides: 5
  summary: '!pagerank'
parallel-ann-with-GPU.pptx:
  num_slides: 11
  summary: '!ipm should include a bit more info on GPUs | Deep learning and GPUs'
parallel-lda.pptx:
  num_slides: 9
  summary: '!gibbs-lda !ipm | Parallelizing LDA'
parallel-sgd.pptx:
  num_slides: 3
  summary: parallel sgd analysis and experiments | Parallel SGD via Param Mixing
parallel-simjoins-wthout-pig.pptx:
  num_slides: 13
  summary: examples in guinea pig !gpig !simjoins | Parallel simjoins
parallel-simjoins.pptx:
  num_slides: 16
  summary: examples in guinea pig and pig !pig !simjoins | Parallel simjoins
parallel-stream-and-sort-aside.pptx:
  num_slides: 5
  summary: 'parallel stream and sort'
parallel-stream-and-sort.pptx:
  num_slides: 10
  summary: Parallelize stream and sort
param-server-intro.pptx:
  num_slides: 13
  summary: +ps | Parameter servers
param-server-ssp.pptx:
  num_slides: 25
  summary: '!ps | State Synchronous Parallel (SSP) model'
param-server-vs-hadoop.pptx:
  num_slides: 3
  summary: '!ps !ipm | PS vs Hadoop'
perceptron+rank+struct-recap.pptx:
  num_slides: 11
  summary: review of perceptrons with ranking and structured perceptron
perceptron-recap.pptx:
  num_slides: 6
  summary: review of perceptrons
phrase-finding-in-hadoop-using-question-response.pptx:
  num_slides: null
  summary: ''
phrase-finding-in-pig-example.pptx:
  num_slides: 27
  summary: 'features: shell-like commands, load, foreach, group, describe, illustrate,
    join, store, cross, udf, !phrases | Phrase-finding in Pig'
phrase-finding-paper.pptx:
  num_slides: 19
  summary: +phrases | 
pic-paper.pptx:
  num_slides: 22
  summary: lots of results - todo, add some stuff on later math/analysis !spectral~average
    +pic | Power iteration clustering
pig-overview.pptx:
  num_slides: 6
  summary: '+pig '
pig-short-overview.pptx:
  num_slides: 4
  summary: bare essentials of pig +pig
powergraph.pptx:
  num_slides: 14
  summary: '!graph-arch !graphlab | PowerGraph'
prediction-game.pptx:
  num_slides: 4
  summary: ''
pregel.pptx:
  num_slides: 6
  summary: '!graph-arch +pregel | Pregel'
ranking-perceptron.pptx:
  num_slides: 11
  summary: ' | Ranking perceptrons'
ranking-to-structured-perceptron.pptx:
  num_slides: 13
  summary: algorithm for NER and collin's experiments | Structured perceptrons
request-and-answer-for-testing-large-classifiers.pptx:
  num_slides: 22
  summary: '!nbayes +nbayes-test | Scalable classification'
request-and-answer-recap.pptx:
  num_slides: 8
  summary: recap of NB request-and-answer, discussion of use for phrases !phrases
rnn-apps.pptx:
  num_slides: 14
  summary: '!deep | Achitectures using RNNs'
rnn.pptx:
  num_slides: 20
  summary: '!deep | Recursive ANNs'
rocchio.pptx:
  num_slides: 14
  summary: rocchio vs naive bayes, results, tfidf naive bayes +rocchio !nbayes | Rocchio
    and TFIDF
scalable-nb-notes.pdf:
  num_slides: 0
  summary: streaming naive bayes; stream-and-sort naive bayes; caching stream-and-sort
    naive bayes; large model testing kludge; look-ahead to hadoop | Notes on scalable
    naive bayes
scalable-rocchio.pptx:
  num_slides: 12
  summary: '!nbayes-test !rocchio !nbayes | Scalable Rocchio and TFIDF'
sgd-debugging-tips.pptx:
  num_slides: 10
  summary: actually fairly general comments about debugging ML algorithms | Debugging
    ML algorithms
sgd-for-logistic-regression.pptx:
  num_slides: 7
  summary: derivation of update +lr-sgd | Logistic regression with SGD
sgd-regularization-and-sparsity.pptx:
  num_slides: 11
  summary: naive algorithm for un-regularized, regularized logreg | Regularized SGD
sgd-vs-streaming.pptx:
  num_slides: 4
  summary: also compare to rocchio and naive bayes !rocchio !nbayes
sgd-vw-experiments.pptx:
  num_slides: 7
  summary: ''
short-managed-comm-in-param-server.pptx:
  num_slides: 28
  summary: -ps +lasso +coord-descent | Managed Communication in PS
short-perceptron-analysis.pptx:
  num_slides: 8
  summary: ''
short-stream-and-sort.pptx:
  num_slides: 16
  summary: stream wc example in Python | Stream and Sort as Ops
short-stream-sort-to-mr.pptx:
  num_slides: 10
  summary: Compressed Sketch of Stream-Sort WC Example and Shuffle-Sort
recap-longer-stream-sort-mr.pptx :
  num_slides: 14
  links:
  - https://xavi.ivars.me/arxius/manuals/bash/unix-for-poets.pdf:
      Ken Church (1994). Unix for Poets
  summary: Python Streaming WC
signal-collect.pptx:
  num_slides: 17
  summary: '!graph-arch !pregel | Signal-collect'
simjoins-complete:
  num_slides: 24
  summary: ' !gpig !tfidf | Similarity Joins'
simjoins-intro.pptx:
  num_slides: 13
  summary: true names quote, motivation, some results using tfidf as a similarity
    metric +simjoins | Similarity joins
simjoins-with-tfidf-basic.pptx:
  num_slides: 4
  summary: definition and some simple optimizations using inverted indices !rocchio
    +simjoins-basic | Similarity joins with TFIDF
simjoins-with-tfidf-optimizations.ptx:
  num_slides: 5
  summary: some simple optimizations using inverted indices !simjoins-basic
slow-learners-are-fast-recap.pptx:
  num_slides: 7
  summary: Langford result on time delayed SGD
slow-learners-are-fast.pptx:
  num_slides: 9
  summary: Langford result on time delayed SGD
sorting-and-pipes.pptx:
  num_slides: 12
  summary: ''
spark-overview.pptx:
  num_slides: 15
  summary: +spark | Spark
spark-details.pptx:
  num_slides: 11
  summary: More about working with Spark
  links:
  - https://github.com/10605/LectureSampleCode/tree/main/spark-workflows:
      code for sample workflows in Spark
  - https://github.com/10605/LectureSampleCode/blob/main/hazsoup/spork_micro.py:
      code for a minimal map-reduce, without distributed processing, using Spark syntax
sparse-averaged-perceptron.pptx:
  num_slides: 8
  summary: ''
sparse-regularized-sgd.pptx:
  num_slides: 9
  summary: ' | Efficient regularized SGD'
sparse-sampling-lda.pptx:
  num_slides: 25
  summary: '!gibbs-lda | Fast sampling for LDA'
spectral-clustering-as-averaging.pptx:
  num_slides: 13
  summary: include shi results and example +spectral~average | Spectral clustering
spectral-clustering-as-optimization.pptx:
  num_slides: 6
  summary: eigenvector stuff and leadin to PIC !spectral~average
spectral-clustering-intro-and-examples.pptx:
  num_slides: 2
  summary: spectral clustering examples
ssl-intro.pptx:
  num_slides: 14
  summary: motivation and general idea with NELL as example +ssl | Semi-supervised
    learning intro
stream-and-sort+combining.pptx:
  num_slides: 7
  summary: using buffers as a 'combiner' for stream-and-sort | Local counting in stream
    and sort
stream-and-sort-alternatives.pptx:
  num_slides: 14
  summary: ' | Alternatives to stream and sort'
stream-and-sort-examples.pptx:
  num_slides: 13
  summary: ' | Stream and sort examples'
stream-and-sort-naive-bayes.pptx:
  num_slides: 5
  summary: '| Naive Bayes using stream-and-sort'
testing-kludge-for-streaming-nb.pptx:
  num_slides: 6
  summary: a kludge for testing a model that doesn't fit in memory on a small test
    set
tfidf-in-abstract-workflow.pptx:
  num_slides: 3
  summary: ''
tfidf-in-guineapig.pptx:
  num_slides: 13
  summary: '!gpig -mapside-join -combiners| TFIDF in Guinea Pig'
tfidf-in-pig.pptx:
  num_slides: 2
  summary: '!pig | TFIDF in Pig'
uses-of-bloom-filters.pptx:
  num_slides: 5
  summary: '!bloom'
  links:
  - https://aclanthology.org/D12-1100.pdf:
      Sketch Algorithms for Estimating Point Queries in NLP - Goyal et al 2012
uses-of-count-min-sketch.pptx:
  num_slides: 7
  summary: '!countmin'
  links:
    - https://aclanthology.org/D12-1100.pdf:
        Sketch Algorithms for Estimating Point Queries in NLP, 2012
vectorize-logreg.pptx:
  num_slides: 29
  summary: ' | Vectorization'
word2vec+glove.pptx:
  num_slides: 6
  summary: '!deep | Word2vec and GloVE'
word2vec.pptx:
  num_slides: 8
  summary: '!deep | Word2vec'
xman.pptx:
  num_slides: 12
  summary: '!autodiff screen shots of xman.py | Breakdown of xman.py'
zookeeper.pptx:
  num_slides: 2
  summary: distributed coordination services
linear-regression.pptx:
  num_slides: 46
  summary: messy version of ginger's slides
recitation-3.pptx:
# Dummy file to hold recitation 3 handout and solutions links
  num_slides: 1
  summary: 'Handouts for Recitation 3'
  links:
  - https://drive.google.com/file/d/1lh9Rcvpuw9iyKbFi8m-iwXd2bwOVPkJO/view?usp=drive_link: Recitation 3 Handout
  - https://drive.google.com/file/d/19DaDoXbClZXIFeyEO-WW3og1odnAL4uX/view?usp=sharing: Solutions for Recitation 3