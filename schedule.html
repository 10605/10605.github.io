<head>
    <meta charset="utf-8">
<title>Schedule for CMU 10-605/10-805 </title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../docs-assets/ico/favicon.png">

    <!-- Bootstrap core CSS -->
    <link href="bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="style.css" rel="stylesheet">

    <!--  HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
</head>

<body>
  <div class="container">
    <div class="panel-footer clearfix">

      <h1>10-605/10-805: ML with Large Datasets, Fall 2025</h1>

      <h3 id="schedule">Schedule</h3>
      This schedule for <a href="https://10605.github.io/">10-605/805</a> is subject to change.

        <div class="table-responsive">
            <table class="table table-bordered table-hover" style="font-size: 13px">
                  <tbody>
                    <tr class="leading" style="background-color: #F0F0F0">
                        <th width="90">Date</th>
                        <th width="60">Class Type</th>
                        <th width="330">Topic</th>
                        <th>Resources</th>
                        <th>Announcements</th>
                    </tr>
                    <tr>
                    </tr>


                    <tr>
                        <td>Mon Aug 25, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Overview <a href="https://drive.google.com/file/d/1hoPMf0hKRY9nk_DY5cJlorN4EJq5t1sp/view?usp=drive_link">/ Slides (pdf)</a> <a href="https://docs.google.com/presentation/d/1u5vYdVO4ZbMTRJ-VRlFb-NJ0yaqcpu-X/edit?usp=drive_link&ouid=108006266785637771510&rtpof=true&sd=true">/ Slides (pptx)</a>  <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0fdf8250-16e0-4f42-b394-b3440113e792">/ Recording</a></td>
                        <td>
                          <ul>
                          <li><a href="https://wwcohen.github.io/postscript/ijcai-93.ps">William W. Cohen (1993). Efficient pruning methods for separate-and-conquer rule learning systems</a>
                          <li><a href="https://aclanthology.org/P01-1005.pdf">Banko, Michele, and Eric Brill (2001). Scaling to very very large corpora for natural language disambiguation</a>
                          <li><a href="https://books.google.com/ngrams/">Google NGrams Viewer</a>
                          <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804817&casa_token=ILyj04MU_08AAAAA:QZtVl6LIhMMKqXSd8moyUfbM2hv4F2Zje-FbQMMvm1vnW0pQz0MK5A4BkQpbXdSr2x8jsULS&tag=1">Norvig, Pereira, Halevy (2009). The Unreasonable Effectiveness of Data</a>
                          <li><a href="https://www.nowpublishers.com/article/Details/MAL-006">Yoshua Bengio (2009). Learning Deep Architectures for AI</a>
                          <li><a href="https://github.com/10605/LectureSampleCode/tree/main/ngrams">code for doing ngram queries</a>
                          <li><a href="https://arxiv.org/abs/2203.15556">Hoffman et al (2022). Training Compute-Optimal LLMs</a>
                          </ul>
                        </td>
                        <td>HW 1 out - Entity Resolution and Naive Bayes in Spark</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Aug 27, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Map-Reduce and Spark <a href="https://drive.google.com/file/d/1pwR78M0Jt0PprnXiBSoSuoDe1XyHw9Ao/view?usp=drive_link">/ Slides (pdf)</a> <a href="https://docs.google.com/presentation/d/1gXUlGgdJcUb05npuRFT7kZ4RdW66LP_i/edit?usp=drive_link&ouid=108006266785637771510&rtpof=true&sd=true">/ Slides (pptx)</a>  <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=8deab927-46a4-4dfe-a052-b3460114a90d">/ Recording</a></td>
                        <td>
                          <ul>
                          <li><a href="https://samwho.dev/numbers/">Visualizing the cost of operations</a>
                          <li><a href="https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage?time=1990..latest">Historical cost of storage</a>
                          <li><a href="https://github.com/erikfrey/bashreduce">code for Bash Reduce - a minimal implementation of Map-Reduce</a>
                          <li><a href="https://github.com/10605/LectureSampleCode/tree/main/hazsoup">code for Hazsoup - a more readable minimal implementation of Map-Reduce</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Aug 29, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation: PySpark    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Sep 3, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Workflows for Map-Reduce Systems <a href="https://drive.google.com/file/d/1pV8xIVsdK8flc1l4A87rEJkcwWFSODEO/view?usp=drive_link">/ Slides (pdf)</a> <a href="https://docs.google.com/presentation/d/1VBUBbove1Y1PAMcRKwBc4WuLMScMoiAq/edit?usp=drive_link&ouid=108006266785637771510&rtpof=true&sd=true">/ Slides (pptx)</a>  </td>
                        <td>
                          <ul>
                          <li><a href="https://github.com/10605/LectureSampleCode/tree/main/spark-workflows">code for sample workflows in Spark</a>
                          <li><a href="https://github.com/10605/LectureSampleCode/blob/main/hazsoup/spork_micro.py">code for a minimal map-reduce, without distributed processing, using Spark syntax</a>
                          <li><a href="https://aclanthology.org/W03-1805.pdf">A Language Model Approach to Keyphrase Extraction</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Sep 5, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation: Linear Algebra Review    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Sep 8, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Learning as Optimization 1 <a href="https://drive.google.com/file/d/1M1fxuQU8G4nwwijmRZBLkDi60IByVp8y/view?usp=drive_link">/ Slides (pdf)</a> <a href="https://docs.google.com/presentation/d/130ZVQB3ukw45Vo7JQrxiccMw1bqNAepW/edit?usp=drive_link&ouid=108006266785637771510&rtpof=true&sd=true">/ Slides (pptx)</a>  </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Sep 10, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Learning as Optimization 2 <a href="https://drive.google.com/file/d/1itBcbq5q1KCxY1iYXm4yFgWhmNwNTmWK/view?usp=drive_link">/ Slides (pdf)</a> <a href="https://docs.google.com/presentation/d/1WtFKKnUCbSZ8QWPZ4yzVhqFwgzwv5GuP/edit?usp=drive_link&ouid=108006266785637771510&rtpof=true&sd=true">/ Slides (pptx)</a>  </td>
                        <td>
                          <ul>
                          <li><a href="https://drive.google.com/file/d/16mnJaGcfhiEXLEcvWywo01Ew4Dpc-coM/view">Previous class lecture on Sept 11 2024</a>
                          <li><a href="https://drive.google.com/file/d/175P2ftDaS9-diVjO7I8oC57PoGQJKg8B/view">Previous class lecture on Sept 16 2024</a>
                          </ul>
                        </td>
                        <td>HW 2 out - Parallel Linear Regression in Spark<br/> HW 1 due</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Sep 12, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>HW1 Writing Session    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Sep 15, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Learning as Optimization 3 <a href="https://drive.google.com/file/d/15GwrI7Ky35G0i7fUDbTyL9yLzNTGYcnr/view?usp=drive_link">/ Slides (pdf)</a> <a href="https://docs.google.com/presentation/d/1ZQNwjbCLYiDJLdG851B8JZzzbROEMAsm/edit?usp=drive_link&ouid=108006266785637771510&rtpof=true&sd=true">/ Slides (pptx)</a>  </td>
                        <td>
                          <ul>
                          <li><a href="https://www.cs.cmu.edu/~wcohen/10-605/notes/sgd-notes.pdf">William's notes on SGD for Logistic Regression and Sparsity</a>
                          <li><a href="https://proceedings.mlr.press/v5/shi09a.html">Hash Kernels, PMLR 2009</a>
                          <li><a href="https://dl.acm.org/doi/abs/10.1145/1553374.1553516">Feature hashing for large scale multitask learning, ICML 2009</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2011/hash/218a0aefd1d1a4be65601cc6ddc1520e-Abstract.html">Hogwild, A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, Rech et al, 2011</a>
                          <li><a href="https://fasttext.cc/">FastText classification library</a>
                          <li><a href="https://arxiv.org/abs/1607.01759">Bag of Tricks for Efficient Text Classification, Joulin et al 2016</a>
                          <li><a href="https://arxiv.org/abs/2003.06307">Communication-Efficient Distributed Deep Learning, Tang et al 2023</a>
                          <li><a href="https://aclanthology.org/N10-1069.pdf">Distributed Training Strategies for the Structured Perceptron, McDonald et al, 2010</a>
                          <li><a href="https://dl.acm.org/doi/abs/10.1145/2020408.2020426">Large-scale matrix factorization with distributed stochastic gradient descent - Gemulla et al 2011</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Sep 17, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Randomized Algorithms 1 - Bloom Filters and Count-Min Sketches <a href="https://drive.google.com/file/d/1ATVOyDt9Sb9PEkn0quMKeBIknENwBYm_/view?usp=drive_link">/ Slides (pdf)</a> <a href="https://docs.google.com/presentation/d/1qglAAnK0lIgNayc9tUCrd0ZvKlauDdbL/edit?usp=drive_link&ouid=108006266785637771510&rtpof=true&sd=true">/ Slides (pptx)</a>  </td>
                        <td>
                          <ul>
                          <li><a href="https://www.jmlr.org/papers/volume15/agarwal14a/agarwal14a.pdf">Agarwal et al, A Reliable Effective Terascale Linear Learning System</a>
                          <li><a href="https://www.cs.cmu.edu/~wcohen/10-605/notes/randomized-algs.pdf">William's Notes on Randomized Algorithms</a>
                          <li><a href="https://github.com/10605/LectureSampleCode/blob/main/randomized/bloomfilter.py">code for Bloom Filter</a>
                          <li><a href="https://openreview.net/pdf?id=S1hsDCNFx">Short and Deep: Sketching and Neural Networks, Daniely et al</a>
                          <li><a href="https://aclanthology.org/D12-1100.pdf">Sketch Algorithms for Estimating Point Queries in NLP, 2012</a>
                          <li><a href="https://drive.google.com/file/d/17OyCk5jMM1Kfjt3MuQDm00DJWJoBCuxi/view">Previous course lecture, 9/25/2024</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Sep 19, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation 3: Probability, Evaluation Metrics and Minhashing <a href="https://drive.google.com/file/d/1lh9Rcvpuw9iyKbFi8m-iwXd2bwOVPkJO/view?usp=sharing">/ Slides (pdf)</a>   </td>
                        <td>
                          <ul>
                          <li><a href="https://drive.google.com/file/d/1lh9Rcvpuw9iyKbFi8m-iwXd2bwOVPkJO/view?usp=drive_link">Recitation 3 Handout</a>
                          <li><a href="https://drive.google.com/file/d/19DaDoXbClZXIFeyEO-WW3og1odnAL4uX/view?usp=sharing">Solutions for Recitation 3</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Sep 22, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Randomized Algorithms 2 - Locality Sensitive Hashing    </td>
                        <td>
                          <ul>
                          <li><a href="https://arxiv.org/pdf/1307.2982">Fast Exact Search in Hamming Space with Multi-Index Hashing, 2014</a>
                          <li><a href="https://arxiv.org/abs/1908.10396">Accelerating Large-Scale Inference with Anisotropic Vector Quantization, 2020</a>
                          <li><a href="https://arxiv.org/pdf/2401.08281">The FAISS library, 2024</a>
                          </ul>
                        </td>
                        <td>HW 3 out - Locality-Sensitive Hashing</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Sep 24, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>HW2 Writing Session / AutoDiff    </td>
                        <td>
                          <ul>
                          <li><a href="https://justindomke.wordpress.com/2009/03/24/a-simple-explanation-of-reverse-mode-automatic-differentiation/">A simple explanation of reverse-mode automatic differentiation, Justin Domke</a>
                          <li><a href="https://github.com/10605/LectureSampleCode/tree/main/autodiff">code for autodiff with Wengert lists</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Sep 26, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation: AWS    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Sep 29, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Parallel Optimization with GPUs 1    </td>
                        <td>
                          <ul>
                          <li><a href="https://www.youtube.com/watch?v=-P28LKWTzrI">Mythbusters explain GPUs</a>
                          <li><a href="https://ourworldindata.org/grapher/gpu-price-performance">GFlops / dollar up to 2022</a>
                          <li><a href="https://github.com/10605/LectureSampleCode/blob/main/autodiff/gpu.py">code for benchmarking GPUs</a>
                          <li><a href="https://drive.google.com/file/d/1-qTFQw7rW1rIsGf8kN4-pxv_edbfFfmp/view?usp=sharing">Fall 2024 10-605 lecture on ML hardware</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Oct 1, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Parallel Optimization with GPUs 2    </td>
                        <td>
                          <ul>
                          <li><a href="https://drive.google.com/file/d/1-MLmngnoJAAgjVm4CpCLXcjm6WQsjFkU/view?usp=sharing">Previous class lecture on Nov 4 2024</a>
                          <li><a href="https://drive.google.com/file/d/1-qTFQw7rW1rIsGf8kN4-pxv_edbfFfmp/view?usp=sharing">Previous class lecture on Oct 21 2024</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Oct 3, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation: Practice Exam    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td>HW 3 due</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Oct 6, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Exam Review and QA    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Oct 8, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Exam 1    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Oct 10, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>No Recitation    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Oct 20, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Deep Learning: Background and Architectures    </td>
                        <td>
                          <ul>
                          <li><a href="http://neuralnetworksanddeeplearning.com/">online book on deep networks</a>
                          <li><a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks, Glorot and Bengio, AIStats 2010</a>
                          <li><a href="https://deeplizard.com/resource/pavq7noze2">Convolution demo</a>
                          <li><a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Deep Residual Learning for Image Recognition, He et al, CVPR 2016</a>
                          <li><a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space, Mikolov et al, 2013</a>
                          <li><a href="https://projector.tensorflow.org/">Visualize Word2Vec</a>
                          <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy blog post on LSTMs</a>
                          <li><a href="https://wtf-deeplearning.github.io/machine-translation/1409.0473.pdf">NMT by jointly learning to align and translate, 2016</a>
                          </ul>
                        </td>
                        <td>HW: TBD</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Oct 22, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Deep Learning: Transformers    </td>
                        <td>
                          <ul>
                          <li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a>
                          <li><a href="https://github.com/rasbt/LLMs-from-scratch">LLMs from scratch book on Github</a>
                          <li><a href="https://e2eml.school/transformers.html">A bit like the annotated transformer but with more depth in some areas</a>
                          <li><a href="https://dl.acm.org/doi/abs/10.5555/2627435.2670313">Dropout paper, Srinivasta et al JMLR 2014</a>
                          <li><a href="https://arxiv.org/abs/1607.06450">LayerNorm paper,  Ba et al, 2016.</a>
                          <li><a href="https://arxiv.org/pdf/1508.07909">Neural Machine Translation of Rare Words with Subword Units, Sennrich et al 2016</a>
                          <li><a href="https://huggingface.co/docs/transformers/tokenizer_summary">Huggingface's guide to BPE and similar</a>
                          <li><a href="https://arxiv.org/pdf/2010.11929/1000">An Image is Worth 16x16 Words: Transformers for Image Recognition, ICLR 2021</a>
                          <li><a href="https://arxiv.org/abs/2210.02928">MuRag Multimodal Retrieval-Augmented Generation ..., Chen et al 2022</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Oct 24, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation: PyTorch / HW4   <a href="https://drive.google.com/file/d/1yuHDmUkg9OOfSY-cCHhL8k88PPgU4fnD/view?usp=sharing">/ Notebook</a> <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=a8ca568d-8444-49f4-ad82-b2a0011f6345">/ Recording</a></td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Oct 27, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Hyperparameter Search    </td>
                        <td>
                          <ul>
                          <li><a href="https://drive.google.com/file/d/1T4s3CPBHazGhGFZpBcLa6HtnvS7PRcMT/view?usp=drive_link">PDF of Greg's slide deck</a>
                          <li><a href="https://arxiv.org/abs/2203.15556">Hoffman et al (2022). Training Compute-Optimal LLMs</a>
                          <li><a href="https://openreview.net/forum?id=xI71dsS3o4">(Mis)fitting - a study of scaling laws, ICLR 2025</a>
                          <li><a href="https://www.jmlr.org/papers/v18/16-558.html">Hyperband - A Novel Bandit Based Approach to Hyperparameter Optimization, JMLR 2018</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Oct 29, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Tokenization and Model compression    </td>
                        <td>
                          <ul>
                          <li><a href="https://arxiv.org/pdf/1508.07909">Neural Machine Translation of Rare Words with Subword Units, Sennrich et al 2016</a>
                          <li><a href="https://huggingface.co/docs/transformers/tokenizer_summary">Huggingface's guide to BPE and similar</a>
                          <li><a href="https://arxiv.org/pdf/2010.11929/1000">An Image is Worth 16x16 Words: Transformers for Image Recognition, ICLR 2021</a>
                          <li><a href="https://arxiv.org/abs/2210.02928">MuRag Multimodal Retrieval-Augmented Generation ..., Chen et al 2022</a>
                          <li><a href="https://medium.com/@joaolages/kv-caching-explained-276520203249">Jo√£o Lages blog post</a>
                          <li><a href="https://arxiv.org/pdf/1503.02531">Distilling the Knowledge in a Neural Network, Hinton, Vinyals, Dean, 2015</a>
                          <li><a href="https://dl.acm.org/doi/pdf/10.1145/1150402.1150464">Model Compression, Bucila et al, 2006</a>
                          <li><a href="https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html">Raschka blog post, 2023</a>
                          <li><a href="https://leimao.github.io/article/Neural-Networks-Quantization/">Lei Mao blog post, 2023</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf">LLM.int8(), Dettmers et al 2022</a>
                          <li><a href="https://openreview.net/forum?id=i8tGb1ab1j">Dettmers and Zettlemoyer 2023</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/091166620a04a289c555f411d8899049-Paper-Conference.pdf">PV-Tuning, Malinovskii et al, 2024</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Oct 31, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation - HW5    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Nov 3, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Quantization and Pruning for LLMs    </td>
                        <td>
                          <ul>
                          <li><a href="https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html">Raschka blog post, 2023</a>
                          <li><a href="https://leimao.github.io/article/Neural-Networks-Quantization/">Lei Mao blog post, 2023</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf">LLM.int8(), Dettmers et al 2022</a>
                          <li><a href="https://openreview.net/forum?id=i8tGb1ab1j">Dettmers and Zettlemoyer 2023</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/091166620a04a289c555f411d8899049-Paper-Conference.pdf">PV-Tuning, Malinovskii et al, 2024</a>
                          <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf">The cat neuron paper</a>
                          <li><a href="https://arxiv.org/pdf/1704.01444">Learning to Generate Reviews and Discovering Sentiment, Radford et al 2017</a>
                          <li><a href="https://arxiv.org/pdf/1710.01878">To prune or not to prune ..., Zhu and Gupta 2017</a>
                          <li><a href="https://arxiv.org/abs/2306.11695">A simple and effective approach ... (the Wanda paper), Sun et al</a>
                          <li><a href="https://arxiv.org/abs/2104.08378">Accelerating Sparse Deep Neural Networks, Mishra et al 2021</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf">LLM Pruner, 2023</a>
                          <li><a href="https://openreview.net/forum?id=18VGxuOdpu">Shortened Llama, ... Kim et al 2024</a>
                          <li><a href="https://arxiv.org/pdf/2310.06694">Sheared LLama ... , Xia et al, 2024</a>
                          <li><a href="https://arxiv.org/pdf/1510.00149">Han et al 2016</a>
                          </ul>
                        </td>
                        <td>HW: GPT-2 Training and Miniproject released</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Nov 5, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>KV-Caching for LLMs    </td>
                        <td>
                          <ul>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf">LLM Pruner, 2023</a>
                          <li><a href="https://openreview.net/forum?id=18VGxuOdpu">Shortened Llama, ... Kim et al 2024</a>
                          <li><a href="https://arxiv.org/pdf/2310.06694">Sheared LLama ... , Xia et al, 2024</a>
                          <li><a href="https://arxiv.org/pdf/1510.00149">Han et al 2016</a>
                          <li><a href="https://arxiv.org/abs/2004.08483">ETC: Encoding Long and Structured Inputs in Transformers, Ainsle et al 2020</a>
                          <li><a href="https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html">Big Bird: Transformers for Longer Sequences, Zaheer et al, 2020</a>
                          <li><a href="https://arxiv.org/abs/2001.04451">Reformer: the Efficient Transformer, Kitaev et al, 2020</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html">H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models, Zhang et al 2023</a>
                          <li><a href="https://arxiv.org/pdf/2309.17453">Efficient streaming LMs with attention sinks, Xiao et al 2024</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/28ab418242603e0f7323e54185d19bde-Abstract-Conference.html">SnapKV: LLM Knows What You are Looking for Before Generation, Li et al 2024</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Nov 7, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>Recitation - Miniproject    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Nov 10, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>KV Caching and Model Compression Recap    </td>
                        <td>
                          <ul>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/28ab418242603e0f7323e54185d19bde-Abstract-Conference.html">SnapKV: LLM Knows What You are Looking for Before Generation, Li et al 2024</a>
                          <li><a href="https://arxiv.org/abs/2310.01801">Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs, Ge et al 2023</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Nov 12, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Contrastive learning and retrieval - Guest Lecture, John Wieting, Google DeepMind    </td>
                        <td>
                          <ul>
                          <li><a href="https://dl.acm.org/doi/abs/10.1145/3078971.3078983">DeepHash paper, 2017</a>
                          <li><a href="https://arxiv.org/pdf/2004.04906v2/1000">DPR paper, 2020</a>
                          <li><a href="https://arxiv.org/pdf/2104.08253">Condenser paper</a>
                          <li><a href="https://arxiv.org/pdf/2109.08535">EntityQuestions paper</a>
                          <li><a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401075">ColBERT paper</a>
                          <li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf">MoCo paper</a>
                          <li><a href="https://arxiv.org/pdf/2112.09118">Contreiver paper</a>
                          </ul>
                        </td>
                        <td>Previous HW due</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Nov 14, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>No Recitation    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Nov 17, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Optimizing Transformer Architectures - Guest Lecture, Michiel de Jong, Cursor    </td>
                        <td>
                          <ul>
                          <li><a href="https://aclanthology.org/2023.acl-long.99.pdf">HyDE paper</a>
                          <li><a href="https://arxiv.org/pdf/2304.14233">LameR paperx</a>
                          <li><a href="https://openreview.net/pdf?id=Ahlrf2HGJR">Echo embedding paper</a>
                          <li><a href="https://arxiv.org/pdf/2307.16645">PromptEOL paper</a>
                          <li><a href="https://arxiv.org/pdf/2005.11401">RAG for knowledge-intensive NLP tasks, 2020</a>
                          <li><a href="https://proceedings.mlr.press/v119/guu20a.html?ref=http://githubhelp.com">REALM paper, 2020</a>
                          <li><a href="https://arxiv.org/abs/2007.01282">Fusion in decoder paper, 2021</a>
                          <li><a href="https://arxiv.org/pdf/2212.08153">FiDO paper</a>
                          <li><a href="https://arxiv.org/pdf/1911.02150">Shazeer paper on optimizing decoder inference with multihead queries</a>
                          <li><a href="https://arxiv.org/pdf/2212.10947">Parallel Context Windows, 2023</a>
                          <li><a href="https://arxiv.org/pdf/2409.15355">Block-Attention for Efficient RAG, 2024</a>
                          <li><a href="https://arxiv.org/pdf/2503.08640">Dynamic Block-Sparse Attention paper, 2025</a>
                          <li><a href="https://arxiv.org/abs/1911.02150">Shazeer paper analyzing operational intensity</a>
                          <li><a href="https://proceedings.mlr.press/v202/de-jong23a/de-jong23a.pdf">LUMEN paper</a>
                          <li><a href="https://arxiv.org/pdf/2306.10231">GLIMMER paper</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Nov 19, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>RAG3 and Embarrasingly Parallel Training    </td>
                        <td>
                          <ul>
                          <li><a href="https://arxiv.org/pdf/2212.10947">Parallel Context Windows, 2023</a>
                          <li><a href="https://arxiv.org/pdf/2409.15355">Block-Attention for Efficient RAG, 2024</a>
                          <li><a href="https://arxiv.org/pdf/2503.08640">Dynamic Block-Sparse Attention paper, 2025</a>
                          <li><a href="https://arxiv.org/pdf/2208.03306">Branch-train-merge paper</a>
                          <li><a href="https://arxiv.org/pdf/2403.07816">Branch-train-mix paper</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Fri Nov 21, 2025</td>
                        <td><strong class="label label-red">Recitation</strong></td>
                        <td>No Recitation    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Nov 24, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Embarassingly Parallel Train 2 and Scalable Evaluation    </td>
                        <td>
                          <ul>
                          <li><a href="https://aclanthology.org/W14-1618.pdf">Levy and Goldberg demystification of word2vec (1/2) 2014</a>
                          <li><a href="https://arxiv.org/pdf/1402.3722">Levy and Goldberg demystification of word2vec (2/2) 2014</a>
                          <li><a href="https://arxiv.org/pdf/2212.04089">Task arithmetic paper, 2023</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/d28077e5ff52034cd35b4aa15320caea-Paper-Conference.pdf">Weight disentanglement and neural tangent kernel paper, 2023</a>
                          <li><a href="https://www.researchgate.net/profile/Leshem-Choshen-2/publication/371291070_Resolving_Interference_When_Merging_Models/links/6481cc4a79a722376518d1d6/Resolving-Interference-When-Merging-Models.pdf">TIES-merging paper, 2023</a>
                          <li><a href="https://arxiv.org/pdf/2307.13269">LoRA hub paper, 2024</a>
                          <li><a href="https://arxiv.org/pdf/2402.05859">PHATGOOSE paper, 2024</a>
                          <li><a href="https://www.cs.cornell.edu/home/kleinber/networks-book/">Information cascades, Ch 16, Easly and Kleinberg, 2010</a>
                          <li><a href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1756-8765.2009.01030.x">Salganic, Dodds, Watts, Science 2006</a>
                          <li><a href="http://nifty.stanford.edu/2014/mccown-schelling-model-segregation/">Schelling Spatial Segregation model</a>
                          <li><a href="https://www.science.org/doi/full/10.1126/science.adi6000?casa_token=UKBrYB_jH6gAAAAA%3AcBNEc67rQIRWJqH_d3O_jFGMdyF3EZlz6R_dhEUQefUIilA5CpzRRHmv5OqBlXQ7YCFTfmM9x8E8bQ">Prediction Powered Inference, Angelopolous 2023, Science</a>
                          <li><a href="https://arxiv.org/abs/2405.06034">Bayesian PPI, Hofer et al 2024</a>
                          <li><a href="https://arxiv.org/pdf/2305.06984">Kamaloo et al, 2023</a>
                          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/c9fcd02e6445c7dfbad6986abee53d0d-Paper-Conference.pdf">Fisch et al, 2024</a>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Mon Dec 1, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Exam Review and QA    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td>Miniproject due</td>
                    </tr>
                    <tr>
                    </tr>
                    <tr>
                        <td>Wed Dec 3, 2025</td>
                        <td><strong class="label label-pink">Lecture</strong></td>
                        <td>Exam 2    </td>
                        <td>
                          <ul>
                          </ul>
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                    </tr>
                </tbody>
            </table>
        </div>
		
        <br /><br />
		

    </div>
    <!-- /container 


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>
    <script>
      anchors.options.visible = 'always';
      anchors.add('.anchored');
    </script>

</body>
